{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55a85359",
   "metadata": {},
   "source": [
    "# Ask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf21f82",
   "metadata": {},
   "source": [
    "For the 10k pids:\n",
    "- what other data types they have, E.g  EHR, 'The Basics', 'Lifestyle', 'Overall Health', 'Personal and Family Health History', 'Social Determinants of Health' surveys, etc.  \n",
    "\n",
    "-  deep dive EHR and characterize it and evaluate how comprehensive it is. May be we can simply populate a figure like this â€œhistorical availability of EHR records (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff6d5e",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3e9caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from postgres_config import db_postgres\n",
    "import os\n",
    "import psycopg2\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()\n",
    "v7_dataset  ='aou-res-curation-output-prod.C2022Q4R11'\n",
    "#cloud_sql_proxy -instances=aou-pdr-data-prod:us-central1:prod-pdr-5deb-lhty=tcp:7000,aou-pdr-data-prod:us-central1:prod-pdr-alpha-replica=tcp:7005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36f817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "allv7_pids = pd.read_gbq(f'''\n",
    "    SELECT DISTINCT p.person_id as research_id\n",
    "    , has_ppi_survey_data AS PPI             \n",
    "    , has_physical_measurement_data AS PM \n",
    "    , has_ehr_data AS EHR\n",
    "    , has_fitbit AS Fitbit                \n",
    "    , has_whole_genome_variant AS WGS\n",
    "    , has_array_data as Arr\n",
    "    , has_lr_whole_genome_variant as lr_wgs\n",
    "    , has_structural_variant_data as strutural_variant\n",
    "    FROM `{v7_dataset}.person` p\n",
    "    LEFT JOIN `{v7_dataset}.cb_search_person` USING(person_id) ''')\n",
    "\n",
    "display(allv7_pids.head(2))\n",
    "allv7_pids.to_csv('v7_ct_participant.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ff20a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data_characterization_multi-omics_pids.xlsx'\n",
    "writer = pd.ExcelWriter(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b156b0",
   "metadata": {},
   "source": [
    "## mapping the 10 pids : biobank_id --> person_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bded6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_10k = pd.read_excel('the_10k_mar2024.xlsx')[['biobank_id']]\n",
    "the_10k['biobank_id'] = [int(p.replace('A','')) for p in the_10k['biobank_id']]\n",
    "the_10k.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dabe25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_pid_map = pd.read_sql('''SELECT DISTINCT research_id as person_id, biobank_id, participant_id FROM pdr.mv_participant''', db_postgres)\n",
    "v7_pids = pd.read_gbq(f'''SELECT DISTINCT person_id FROM `{v7_dataset}.person`''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914369f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "the10k_mapped = the_10k.merge(bio_pid_map)#.merge(v7_pids)\n",
    "the10k_mapped.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc30d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(the10k_mapped.person_id) in set(v7_pids.person_id)\\\n",
    ", set(the10k_mapped.person_id) == set(the10k_mapped.merge(v7_pids).person_id)\\\n",
    ", the10k_mapped.person_id.nunique(), the_10k.biobank_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1c6b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(the10k_mapped.person_id) - set(v7_pids.person_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb208f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## only those of the 10 k that are in v7 (503 bids are not in v7)\n",
    "v7_10k_mapped = the10k_mapped.merge(v7_pids)\n",
    "v7_10k_mapped.person_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c85687",
   "metadata": {},
   "outputs": [],
   "source": [
    "v7_10k_mapped_tp = tuple(v7_10k_mapped.person_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dedca3",
   "metadata": {},
   "source": [
    "# What dtypes do they have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b32392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile overall_summary.py\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# For Data Visualization\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from venn import venn\n",
    "#from utilities import utilities as u\n",
    "\n",
    "class overall_summary:    \n",
    "    ## Class to get ubr flags for each person in the current dataset    \n",
    "    def __init__(self):\n",
    "        self.dataset = 'aou-res-curation-output-prod.C2022Q4R11'\n",
    "        \n",
    "    def client_read_gbq(self, query, jobconfig = None):\n",
    "        # function to read data from BQ into py dataframe with the client\n",
    "        if jobconfig is None: job_config = bigquery.QueryJobConfig(default_dataset=self.dataset)\n",
    "        else: job_config = jobconfig\n",
    "        query_job = client.query(query, job_config =job_config)  # API request\n",
    "        df = query_job.result().to_dataframe()\n",
    "        return df\n",
    "        \n",
    "#     def aian_pids(self):\n",
    "#         aian_pids = self.client_read_gbq(f'''SELECT DISTINCT person_id FROM `ds_survey`\n",
    "#                                 WHERE question_concept_id = 1586140 AND answer = 'What Race Ethnicity: AIAN' ''')\n",
    "#         return aian_pids\n",
    "    \n",
    "#     def wear_pids(self):\n",
    "#         wear_pids = self.client_read_gbq(f'''SELECT DISTINCT person_id \n",
    "#                                         FROM `wear_study` WHERE resultsconsent_wear= 'Yes' ''')\n",
    "#         return wear_pids\n",
    "\n",
    "    def data_types(self, pids):\n",
    "    \n",
    "        # Query to get a list of all participants and their data types\n",
    "        data_types_df = self.client_read_gbq(f\"\"\"\n",
    "            SELECT \n",
    "                DISTINCT p.person_id\n",
    "                , has_ppi_survey_data AS PPI             \n",
    "                , has_physical_measurement_data AS PM \n",
    "                , has_ehr_data AS EHR\n",
    "                , has_fitbit AS Fitbit                \n",
    "                , has_whole_genome_variant AS WGS\n",
    "                , has_array_data as Arr\n",
    "                ---, has_lr_whole_genome_variant as lr_wgs\n",
    "                --, has_structural_variant_data as strutural_variant\n",
    "                --, CASE WHEN has_whole_genome_variant = 1 OR has_array_data = 1 THEN 1 END AS has_wgs_or_array\n",
    "                --, CASE WHEN has_whole_genome_variant = 1 AND has_array_data = 1 THEN 1 END AS has_wgs_and_array\n",
    "\n",
    "            FROM `person` p\n",
    "            LEFT JOIN `cb_search_person` USING(person_id) \n",
    "            WHERE person_id IN {pids}\n",
    "            \n",
    "            \"\"\")\n",
    "        data_types_df = data_types_df.rename(columns = {'Arr':\"Array\"})\n",
    "        data_types_df = data_types_df.fillna(0).astype('int64')\n",
    "\n",
    "        return data_types_df\n",
    "    \n",
    "    def add_on_data_types(self, pids):\n",
    "    \n",
    "        # Query to get a list of all participants and their data types\n",
    "        data_types_df = self.client_read_gbq(f\"\"\"\n",
    "            SELECT \n",
    "                DISTINCT p.person_id\n",
    "                , has_lr_whole_genome_variant as lr_wgs\n",
    "                , has_structural_variant_data as strutural_variant\n",
    "                , CASE WHEN has_whole_genome_variant = 1 OR has_array_data = 1 THEN 1 END AS has_wgs_or_array\n",
    "                , CASE WHEN has_whole_genome_variant = 1 AND has_array_data = 1 THEN 1 END AS has_wgs_and_array\n",
    "\n",
    "            FROM `person` p\n",
    "            JOIN `cb_search_person` USING(person_id) \n",
    "            WHERE person_id IN {pids}\"\"\")\n",
    "        data_types_df = data_types_df.fillna(0).astype('int64')\n",
    "\n",
    "        return data_types_df\n",
    "\n",
    "    def calc_percentage(self, df, denom):\n",
    "        df['% of total participants in CDR'] = df['Count of Participants in CDR']/denom\n",
    "        df['% of total participants in CDR'] = ['{:.2%}'.format(i) for i in df['% of total participants in CDR']]\n",
    "\n",
    "        return df\n",
    "\n",
    "    def count_by_data_type(self, df):\n",
    "        #display(data_types_df)\n",
    "        denom = df.person_id.nunique()\n",
    "        \n",
    "        DF = df.melt(id_vars = ['person_id'])\n",
    "        DF = DF[DF.value == 1].drop('value', axis =1)\n",
    "\n",
    "        DF_none = df.loc[df[df.drop('person_id', axis =1).columns].sum(axis = 1) == 0][['person_id']]\n",
    "        DF_none['variable'] = 'None'\n",
    "\n",
    "        DF = pd.concat([DF, DF_none])\n",
    "\n",
    "        DF_count = DF.groupby(['variable'], as_index = False).nunique().sort_values('person_id')\n",
    "        DF_count.columns = ['Data Type','Count of Participants in CDR']\n",
    "\n",
    "        DF_count = self.calc_percentage(DF_count, denom = denom)\\\n",
    "            .rename(columns = {'Count of Participants in CDR':'Count of Participants in CDR ('+'{:,}'.format(denom)+')'})\n",
    "\n",
    "        return DF_count\n",
    "\n",
    "\n",
    "    def count_by_n_data_type(self, df):\n",
    "        \n",
    "        denom = df.person_id.nunique()\n",
    "        n_data_types_df = pd.DataFrame(df.set_index('person_id').sum(axis = 1)\n",
    "                                      , columns = ['n_data_types']).reset_index()\n",
    "\n",
    "\n",
    "        ## By number of data type\n",
    "        DF_count = n_data_types_df.groupby(['n_data_types'], as_index = False).nunique()\n",
    "        DF_count.columns = ['Number of Data Types', 'Count of Participants in CDR']\n",
    "\n",
    "        max_n = int(DF_count['Number of Data Types'].max())\n",
    "        DF_count['Number of Data Types'] = ['Participants with \\nONLY '+str(int(i))+' Data Type(s)' \\\n",
    "                                            for i in DF_count['Number of Data Types']]   \n",
    "        DF_count = pd.concat([DF_count\n",
    "                              , pd.DataFrame({'Number of Data Types': 'TOTAL PARTICIPANTS IN CDR'\n",
    "                                              , 'Count of Participants in CDR':DF_count['Count of Participants in CDR'].sum()}\n",
    "                                             , index = [''])])\n",
    "\n",
    "        DF_count = self.calc_percentage(DF_count, denom = denom)\n",
    "        DF_count['Number of Data Types'] = DF_count['Number of Data Types']\\\n",
    "                                                .replace({'Participants with \\nONLY 0 Data Type(s)':'Participants with \\n0 Data Type(s)'\n",
    "                                                 , 'Participants with \\nONLY '+str(max_n)+' Data Type(s)':'Participants with \\nall '\\\n",
    "                                                  +str(max_n)+' Data Type(s)'\n",
    "                                                     })\n",
    "\n",
    "        ## By number and type of Data\n",
    "        DF_none = n_data_types_df[n_data_types_df.n_data_types ==0]\n",
    "        DF_none['data_types'] = 'None'\n",
    "\n",
    "        DF = df.melt(id_vars = ['person_id'])\n",
    "        DF = DF[DF.value ==1].drop('value', axis =1)\n",
    "        DF = DF.merge(n_data_types_df[n_data_types_df.n_data_types >0])\n",
    "\n",
    "        DF['data_types'] = DF.groupby(['person_id','n_data_types'])['variable'].transform(lambda x: ' and '.join(x))\n",
    "        DF = DF[['person_id','data_types']].merge(n_data_types_df[n_data_types_df.n_data_types >0])\n",
    "\n",
    "        DF_count_d = pd.concat([DF, DF_none])\n",
    "        DF_count_d = DF_count_d.groupby(['n_data_types','data_types'], as_index = False).nunique()\n",
    "        DF_count_d.columns = ['Number of Data Types','Data Types', 'Count of Participants in CDR']\n",
    "\n",
    "\n",
    "        DF_count_d['Number of Data Types'] = ['Participants with ONLY '+str(int(i))+' Data Type(s)' for i in DF_count_d['Number of Data Types']]\n",
    "\n",
    "        DF_count_d = pd.concat([DF_count_d\n",
    "                                , pd.DataFrame({\n",
    "                                               'Number of Data Types': ' '\n",
    "                                               , 'Count of Participants in CDR':DF_count_d['Count of Participants in CDR'].sum()\n",
    "                                               , 'Data Types': 'TOTAL PARTICIPANTS IN CDR'\n",
    "                                               }, index = [''])])\n",
    "\n",
    "        DF_count_d = self.calc_percentage(DF_count_d, denom = denom)\n",
    "\n",
    "        DF_count_d['Number of Data Types'] = DF_count_d['Number of Data Types']\\\n",
    "                        .replace({'Participants with ONLY 0 Data Type(s)':'Participants with 0 Data Types'\n",
    "                                 , 'Participants with ONLY '+str(max_n)+' Data Type(s)':'Participants with all '\\\n",
    "                                  +str(max_n)+' Data Type(s)'\n",
    "                                 })\n",
    "        DF_count_d['Count of Participants in CDR'] = DF_count_d['Count of Participants in CDR'].astype('int')\n",
    "\n",
    "        return DF_count, DF_count_d\n",
    "\n",
    "    def format_numbers(self, df):\n",
    "        # format the counts and percentage columns\n",
    "\n",
    "        formated_df = df.copy()\n",
    "        for col in formated_df.columns:\n",
    "            if formated_df[col].dtype == 'int64':\n",
    "                formated_df[col] = ['{:,}'.format(i) for i in formated_df[col]]\n",
    "\n",
    "        return formated_df\n",
    "\n",
    "    def combine_count_and_perc(self, df, count_perc_cols_dic, drop = True):    \n",
    "        df_comb = df.copy()\n",
    "\n",
    "        for count_col in count_perc_cols_dic:\n",
    "            perc_col = count_perc_cols_dic[count_col]\n",
    "            df_comb[count_col+'_str'] = df_comb[count_col]\n",
    "            df_comb[perc_col+'_str'] = df_comb[perc_col]\n",
    "            df_comb[[count_col+'_str', perc_col+'_str']] = self.format_numbers(df_comb[[count_col+'_str', perc_col+'_str']])\n",
    "            new_col_name = count_col+' (and %)'\n",
    "            df_comb[new_col_name] = df_comb[count_col+'_str'].astype('str')+' ('+df_comb[perc_col+'_str'].astype('str')+')'\n",
    "\n",
    "            if drop == True:\n",
    "                df_comb = df_comb.drop([count_col, count_col+'_str', perc_col, perc_col+'_str'], axis =1)\n",
    "        return df_comb\n",
    "\n",
    "    def format_dataframe(self, df):\n",
    "\n",
    "        display_DF = pd.DataFrame()\n",
    "        for n in df['Number of Data Types'].unique():\n",
    "            DF = df[df['Number of Data Types'] == n].drop('Number of Data Types', axis =1)\n",
    "            DF = DF.reindex([n]+ DF.index.tolist()).fillna('')\n",
    "            display_DF = pd.concat([display_DF, DF])\n",
    "\n",
    "        index_list = display_DF.index.tolist()\n",
    "        index_list = [i for i in index_list if type(i) == int]\n",
    "\n",
    "        dic = {0:''}\n",
    "        for i in index_list:\n",
    "            dic.update({i:''})\n",
    "        display_DF = display_DF.rename(index=dic)\n",
    "        return display_DF\n",
    "\n",
    "\n",
    "    def combine_final_tables(self, dfs_to_combine, indexname, save = True):\n",
    "        table = pd.DataFrame()\n",
    "        for colname, df in dfs_to_combine.items():\n",
    "            DF = df.set_index(indexname)\n",
    "            DF.columns = [colname]\n",
    "            table = pd.concat([table,DF],axis =1).fillna('-')\n",
    "            \n",
    "                ###################### save the file to the bucket\n",
    "        if save == True:\n",
    "            add = indexname[0].lower().replace(' ', '_')\n",
    "            filename = f'1_count_by_{add}.xlsx'\n",
    "            u().write_to_csv_excel(table, filename = filename)\n",
    "            #print(f'Saving the file to the bucket as {filename}')\n",
    "            #table.to_excel(filename)\n",
    "            #args = [\"gsutil\", \"cp\", f\"./{filename}\", f\"{self.bucket}/notebooks/\"]\n",
    "            #output = subprocess.run(args, capture_output=True)\n",
    "            #print(output.stderr)\n",
    "\n",
    "        return table\n",
    "    \n",
    "    def table_1_by_pop(self, pop_df):\n",
    "        n = pop_df.person_id.nunique()\n",
    "        table1_1_uncomb = self.count_by_data_type(df = pop_df)\n",
    "        table1_2_uncomb, table1_2_raw = self.count_by_n_data_type(df = pop_df)\n",
    "\n",
    "        count_col = \"Count of Participants in CDR\"; perc_col = \"% of total participants in CDR\"\n",
    "        table1_1 = self.combine_count_and_perc(table1_1_uncomb\n",
    "                                        , count_perc_cols_dic= dict({f\"{count_col} ({'{:,}'.format(n)})\":f'{perc_col}'}))\n",
    "        table1_2 = self.combine_count_and_perc(table1_2_raw, count_perc_cols_dic= dict({f'{count_col}':f'{perc_col}'}))\n",
    "        table1_2 = table1_2.reset_index(drop = True)\n",
    "\n",
    "        return table1_1, table1_2, table1_1_uncomb, table1_2_uncomb\n",
    "\n",
    "    def venn_plot(self, data_types_df, pid_type, figname, genomics = 'no'):\n",
    "        ppi_set = set(data_types_df[data_types_df.PPI == 1].person_id)\n",
    "        ehr_set = set(data_types_df[data_types_df.EHR == 1].person_id)\n",
    "        pm_set = set(data_types_df[data_types_df.PM == 1].person_id)\n",
    "        fitbit_set = set(data_types_df[data_types_df.Fitbit == 1].person_id)\n",
    "\n",
    "        data = {'PPI': ppi_set\n",
    "            , 'EHR': ehr_set#.intersection(ppi_1_3_set)\n",
    "            , 'Phys. Meas.': pm_set\n",
    "            , 'Fitbit': fitbit_set}\n",
    "\n",
    "        if genomics.lower() == 'yes':\n",
    "            genomics_set = set(data_types_df[(data_types_df.Array == 1) | (data_types_df.WGS ==1)].person_id)\n",
    "            data.update({'Genomics': genomics_set})\n",
    "\n",
    "        v = venn(data)\n",
    "        plt.title(f'Count of {pid_type} participants with multiple data types \\n', fontsize = 18)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{figname}.jpeg')\n",
    "\n",
    "    def combine_plot_data(self, xcol, dfs_to_combine):\n",
    "\n",
    "        count_col = \"Count of Participants in CDR\"\n",
    "        perc_col = \"% of total participants in CDR\" \n",
    "\n",
    "        ## whole cdr\n",
    "        kw = 'Whole CDR'\n",
    "        df_cdr_raw = dfs_to_combine[kw]\n",
    "        df_cdr = df_cdr_raw.copy()\n",
    "        df_cdr['Participant'] = kw\n",
    "        df_cdr = df_cdr.rename(columns = {f\"{[c for c in df_cdr.columns if 'Count of Participants' in c][0]}\":f'{count_col}'})\n",
    "\n",
    "\n",
    "        DF = pd.DataFrame()\n",
    "        for key in [k for k in dfs_to_combine.keys() if k != 'Whole CDR']:\n",
    "            df = dfs_to_combine[key]\n",
    "            n = df.iloc[-1,1]\n",
    "            df['Number of Data Types'] = df[xcol]#.replace('Participants with \\nall 5 Data Type(s)','Participants with \\nONLY 5 Data Type(s)')\n",
    "            rename_c = [c for c in df.columns if f'{count_col}' in c][0]\n",
    "            df = df.rename(columns = {rename_c:f'{count_col}'})\n",
    "\n",
    "            missing_rows = set(df_cdr[xcol]) - set(df[xcol])\n",
    "            none = pd.DataFrame(missing_rows, columns = [xcol])\n",
    "            none[f'{count_col}'] = 0\n",
    "            none[f'{perc_col}'] = '0%'\n",
    "            none[xcol] = 'Participants with \\n0 Data Type(s)'\n",
    "\n",
    "            df = pd.concat([none, df]).reset_index(drop = True)\n",
    "            df['Participant'] = key\n",
    "            DF = pd.concat([DF, df]).drop_duplicates()\n",
    "\n",
    "        DF = pd.concat([df_cdr, DF]).drop_duplicates()       \n",
    "        return DF\n",
    "\n",
    "    def create_plot_label(self, df_plot, count_col ='Count of Participants in CDR'\n",
    "                         , perc_col = '% of total participants in CDR'):\n",
    "        df_plot_final = df_plot.drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "        df_plot_final[perc_col] = [round(float(c.replace('%', ''))) for c in df_plot_final[perc_col]]\n",
    "        df_plot_final['label'] = ['{:,}'.format(i) for i in df_plot_final[count_col]] \n",
    "        df_plot_final['label'] = df_plot_final['label']+ ' ('+ df_plot_final[perc_col].astype('str')+ '%)'\n",
    "        df_plot_final['label'] = [c.replace(' (', '\\n(') for c in df_plot_final['label']]\n",
    "        return df_plot_final\n",
    "\n",
    "    def bar_plot(self, df, title, label, y, x= 'Data Type', hue= 'Participant'\n",
    "                     , w = 14, h = 10, r = 0, ha = 'left', va = 'bottom'):\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(w,h), tight_layout=True)\n",
    "\n",
    "        ax = sns.barplot(x= x, y = y\n",
    "                             , hue = hue\n",
    "                             , palette='Blues', ci=None\n",
    "                             , edgecolor = 'w', data = df)\n",
    "\n",
    "        for p in ax.patches:\n",
    "            n = int(p.get_height())\n",
    "            l = df.loc[df[y] == n, label].reset_index(drop = True)[0]\n",
    "\n",
    "            t = ax.annotate(l, xy = (p.get_x(), n))\n",
    "            t.set(color = \"black\", size = 12, rotation=r, horizontalalignment= ha, verticalalignment=va)\n",
    "\n",
    "        ax.set(xlabel=\"\",ylabel=\"\")\n",
    "        ax.set_title(title+'\\n\\n', fontsize=18)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=r, horizontalalignment= 'center', fontsize=12)\n",
    "        ax.set(yticklabels=[])\n",
    "\n",
    "        plt.savefig(title.replace('Count of Participants in the CDR ', 'Barplot ')+'.jpeg')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ba133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Data\n",
    "s = overall_summary()\n",
    "data_types_df = s.data_types(pids = v7_10k_mapped_tp)\n",
    "data_types_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0161288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types_df[(data_types_df.Array == 1) | (data_types_df.WGS == 1)].person_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32be423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_data_types_df = s.add_on_data_types(pids = v7_10k_mapped_tp)\n",
    "add_data_types_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0f808",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_data_types_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6472127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_data_types_df[(add_data_types_df.has_wgs_or_array == 1) | (add_data_types_df.strutural_variant == 1) | (add_data_types_df.lr_wgs == 1)].person_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd206242",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deliverable table 1\n",
    "sum_reports_cdr = s.table_1_by_pop(data_types_df)\n",
    "sum_reports_cdr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de364075",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_reports_cdr[0].to_excel(writer, 'Data Availability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a651fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deliverable plot 1\n",
    "s.venn_plot(data_types_df, pid_type = \"CDR\",figname = 'WholeCDR_gen_Venn', genomics= 'yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a59354",
   "metadata": {},
   "source": [
    "# EHR data characterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d33147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ehr_query= f\"\"\"\n",
    "\n",
    "WITH ehr AS (\n",
    "    SELECT DISTINCT person_id, 1 as has_ehr, 'measurement' as domain\n",
    "    FROM `measurement` \n",
    "    JOIN `measurement_ext` USING(measurement_id)\n",
    "    WHERE LOWER(src_id) LIKE 'ehr site%'\n",
    "\n",
    "    UNION DISTINCT\n",
    "    SELECT DISTINCT person_id, 1 as has_ehr, 'condition' as domain\n",
    "    FROM `condition_occurrence` \n",
    "    JOIN `condition_occurrence_ext` USING(condition_occurrence_id)\n",
    "    WHERE LOWER(src_id) LIKE 'ehr site%'\n",
    "\n",
    "    UNION DISTINCT\n",
    "    SELECT DISTINCT person_id, 1 as has_ehr, 'device' as domain\n",
    "    FROM `device_exposure` \n",
    "    JOIN `device_exposure_ext` USING(device_exposure_id)\n",
    "    WHERE LOWER(src_id) LIKE 'ehr site%'\n",
    "\n",
    "    UNION DISTINCT\n",
    "    SELECT DISTINCT person_id, 1 as has_ehr, 'drug' as domain\n",
    "    FROM `drug_exposure` \n",
    "    JOIN `drug_exposure_ext` USING(drug_exposure_id)\n",
    "    WHERE LOWER(src_id) LIKE 'ehr site%'\n",
    "\n",
    "    UNION DISTINCT\n",
    "    SELECT DISTINCT person_id, 1 as has_ehr, 'observation' as domain\n",
    "    FROM `observation` \n",
    "    JOIN `observation_ext` USING(observation_id)\n",
    "    WHERE LOWER(src_id) LIKE 'ehr site%'\n",
    "\n",
    "    UNION DISTINCT\n",
    "    SELECT DISTINCT person_id, 1 as has_ehr, 'procedure' as domain\n",
    "    FROM `procedure_occurrence` \n",
    "    JOIN `procedure_occurrence_ext` USING(procedure_occurrence_id)\n",
    "    WHERE LOWER(src_id) LIKE 'ehr site%' \n",
    "\n",
    "    UNION DISTINCT\n",
    "    SELECT DISTINCT person_id, 1 as has_ehr, 'visit' as domain\n",
    "    FROM `visit_occurrence` \n",
    "    JOIN `visit_occurrence_ext` USING(visit_occurrence_id)\n",
    "    WHERE LOWER(src_id) LIKE 'ehr site%'\n",
    ")\n",
    "\n",
    "SELECT DISTINCT ehr.* \n",
    "FROM ehr\n",
    "WHERE person_id IN {v7_10k_mapped_tp}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "ehr_df = s.client_read_gbq(ehr_query)\n",
    "ehr_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b9bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_ehr = set(v7_10k_mapped.person_id) - set(ehr_df.person_id)\n",
    "len(no_ehr), ehr_df.person_id.nunique(), data_types_df[data_types_df.person_id.isin(no_ehr)].EHR.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b40edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ehr_df.person_id.nunique(), ehr_df.person_id.nunique()+1566"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143213e3",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42115b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ehr_df.domain.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c44dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "ehr_pv_df = ehr_df.pivot(index=['person_id'], columns = ['domain']).fillna(0).reset_index()\n",
    "ehr_pv_df.columns = [c[1] for c in ehr_pv_df.columns]\n",
    "ehr_pv_df = ehr_pv_df.rename(columns = {ehr_pv_df.columns[0]:'person_id'})\n",
    "ehr_pv_bin_df = pd.concat([ehr_pv_df\n",
    "                          , data_types_df[data_types_df.person_id.isin(no_ehr)][['person_id']].drop_duplicates()\n",
    "                          ]).fillna(0)\n",
    "ehr_pv_bin_df          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d3bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ehr_sum_reports_cdr = s.table_1_by_pop(ehr_pv_bin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bebe54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "85+15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bafe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deliverable table 2\n",
    "ehr_sum_reports_cdr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3857e86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ehr_sum_reports_cdr[0].to_excel(writer, 'Data Availability_EHR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6165272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_reports_cdr2 = ehr_sum_reports_cdr[1]\n",
    "sum_reports_cdr2['Number of Data Types'] = [i.replace('Participants with ', '').replace('Data Type', 'EHR Domain')\\\n",
    "                                            .replace('0 EHR Domains','No EHR Data')\n",
    "                                            for i in sum_reports_cdr2['Number of Data Types']]\n",
    "sum_reports_cdr2 = sum_reports_cdr2.set_index(['Number of Data Types', 'Data Types'])\n",
    "sum_reports_cdr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_reports_cdr3 = ehr_sum_reports_cdr[3]\n",
    "sum_reports_cdr3['Number of Data Types'] = [i.replace('Participants with ', '').replace('Data Type', 'EHR Domain')\\\n",
    "                                            .replace('0 EHR Domains','No EHR Data')\n",
    "                                            for i in sum_reports_cdr3['Number of Data Types']]\n",
    "sum_reports_cdr3 = sum_reports_cdr3.set_index(['Number of Data Types'])\n",
    "#sum_reports_cdr3.to_excel('add.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe9d5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_reports_cdr3.to_excel(writer, 'EHR Characterization')\n",
    "sum_reports_cdr2.to_excel(writer, 'EHR Characterization - Details')\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1397b2",
   "metadata": {},
   "source": [
    "## historic ehr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be8e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Data Manipulation\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import statistics\n",
    "import datetime \n",
    "# For Data Visualization\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "#import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class historic_ehr:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.dataset = 'aou-res-curation-output-prod.C2022Q4R11'\n",
    "        self.pids = v7_10k_mapped_tp\n",
    "        \n",
    "    def client_read_gbq(self, query, jobconfig = None):\n",
    "        # function to read data from BQ into py dataframe with the client\n",
    "        if jobconfig is None: job_config = bigquery.QueryJobConfig(default_dataset=self.dataset)\n",
    "        else: job_config = jobconfig\n",
    "        query_job = client.query(query, job_config =job_config)  # API request\n",
    "        df = query_job.result().to_dataframe()\n",
    "        return df\n",
    "        \n",
    "    def ehr_by_domain(self, domain, start_date_field, end_date_field, table, table_id, cutoff):\n",
    "\n",
    "        concept_id = domain.lower()+'_concept_id'\n",
    "            #print(domain, start_date_field, end_date_field, table, table_id, concept_id)\n",
    "        df = self.client_read_gbq(f'''\n",
    "                SELECT DISTINCT person_id, '{domain}' as ehr_domain, src_id AS ehr_site\n",
    "                , vocabulary_id as vocabulary, MIN({start_date_field}) AS start_date\n",
    "                , CASE WHEN MAX({end_date_field}) IS NULL THEN \"{cutoff}\" ELSE MAX({end_date_field}) END AS end_date\n",
    "\n",
    "                FROM `{table}`\n",
    "                JOIN `{table}_ext` USING({table_id})\n",
    "                JOIN `concept` on concept_id = {concept_id}\n",
    "                WHERE LOWER(src_id) LIKE 'ehr site%' AND person_id in {self.pids}\n",
    "                GROUP BY 1,2,3,4''')\n",
    "        return df\n",
    "    \n",
    "    def historic_ehr_data(self):\n",
    "        # setting variables\n",
    "        tables = ['measurement', 'condition_occurrence','device_exposure','drug_exposure'\n",
    "                      ,'observation','procedure_occurrence', 'visit_occurrence']\n",
    "        start_end_date_fields = [['measurement_date', 'measurement_date']\n",
    "                                     , ['condition_start_date','condition_end_date']\n",
    "                                     , ['device_exposure_start_date','device_exposure_end_date']\n",
    "                                     , ['drug_exposure_start_date','drug_exposure_end_date']\n",
    "                                     , ['observation_date','observation_date']\n",
    "                                     , ['procedure_date','procedure_date']\n",
    "                                     , ['visit_start_date','visit_end_date']\n",
    "                                    ]\n",
    "        domains = ['Measurement', 'Condition','Device','Drug', 'Observation','Procedure', 'Visit']\n",
    "        table_ids = ['measurement_id', 'condition_occurrence_id','device_exposure_id', 'drug_exposure_id'\n",
    "                         ,'observation_id', 'procedure_occurrence_id','visit_occurrence_id']\n",
    "\n",
    "\n",
    "        ehr_cutoff_date = self.client_read_gbq(f'''SELECT ehr_cutoff_date  FROM `._cdr_metadata`''')\n",
    "        ehr_cutoff =str(ehr_cutoff_date.ehr_cutoff_date[0])   \n",
    "\n",
    "        historic_ehr_raw = pd.DataFrame()\n",
    "\n",
    "        for i in range(len(tables)):\n",
    "            df = self.ehr_by_domain(domain = domains[i]\n",
    "                                    , start_date_field = start_end_date_fields[i][0]+'time'\n",
    "                                    , end_date_field = start_end_date_fields[i][1]+'time'\n",
    "                                    #, dataset = self.dataset\n",
    "                                    , table = tables[i], table_id = table_ids[i]\n",
    "                                    , cutoff = ehr_cutoff)\n",
    "            historic_ehr_raw = pd.concat([historic_ehr_raw, df])\n",
    "\n",
    "        historic_overall = historic_ehr_raw.copy()\n",
    "        historic_overall['ehr_domain'] = 'Any EHR'\n",
    "        historic_overall = historic_overall.drop_duplicates()#.groupby(['person_id','ehr_domain'], as_index= False).min()\n",
    "        historic_ehr = pd.concat([historic_ehr_raw, historic_overall])\n",
    "\n",
    "        return historic_ehr\n",
    "\n",
    "class historic_ehr_summary:\n",
    "\n",
    "    def __init__(self, denominator, historic_ehr_df):\n",
    "        self.denominator = denominator\n",
    "        self.historic_ehr_df = historic_ehr_df\n",
    "             \n",
    "    def years_of_data(self, historic_ehr_df):\n",
    "        ## Historic EHR data by Domains over time, dataframe \n",
    "        df = historic_ehr_df.reset_index(drop = True)\n",
    "        df = df[['person_id','ehr_domain','start_date', 'end_date']].drop_duplicates()\n",
    "        ###\n",
    "        end_date_max = df[['person_id','ehr_domain','end_date']].drop_duplicates()\\\n",
    "                            .groupby(['person_id','ehr_domain'], as_index = False).max()\n",
    "\n",
    "        start_date_min = df[['person_id','ehr_domain','start_date']].drop_duplicates()\\\n",
    "                            .groupby(['person_id','ehr_domain'], as_index = False).min()\n",
    "\n",
    "        df1 = df.drop(['start_date','end_date'],1).merge(start_date_min, 'left')\\\n",
    "                                .merge(end_date_max, 'left').drop_duplicates()\n",
    "\n",
    "        df1['days_diff'] = df1['end_date'] - df1['start_date']\n",
    "        df1['days_diff'] = [abs(i.days) for i in df1['days_diff']] \n",
    "\n",
    "\n",
    "        df1.loc[df1['days_diff'] == 0, 'days_diff'] = 1\n",
    "        \n",
    "        ###\n",
    "        df_years_of_data = df1[['person_id','ehr_domain','days_diff']]\\\n",
    "                                .groupby(['person_id','ehr_domain'], as_index = False).sum()\n",
    "        df_years_of_data['years_of_data'] = df_years_of_data['days_diff']/365\n",
    "        df_years_of_data = df_years_of_data.sort_values('years_of_data')\n",
    "        years_of_data_per_pid = df_years_of_data.drop('days_diff',1)\n",
    "        \n",
    "        return years_of_data_per_pid\n",
    "    \n",
    "\n",
    "    def pid_cumcount_by_vars(self, df, cum_count_by):\n",
    "\n",
    "        cummulative_counts_df = pd.DataFrame()\n",
    "\n",
    "        for d in df[cum_count_by[0]].unique():\n",
    "            #print('\\n'+d)\n",
    "            DF = df[df[cum_count_by[0]] == d][['person_id']+cum_count_by].sort_values(cum_count_by, ascending = True)\n",
    "            DF['cumcount'] = (~DF['person_id'].duplicated()).cumsum()\n",
    "            cum_df = DF.drop('person_id',1).drop_duplicates().reset_index(drop = True)\n",
    "            cum_df[cum_count_by[0]] = d\n",
    "            cum_df = cum_df.sort_values(cum_count_by).groupby(cum_count_by, as_index = False).last() \n",
    "\n",
    "            cummulative_counts_df = pd.concat([cummulative_counts_df, cum_df])\n",
    "\n",
    "        cummulative_counts_df['cumcount'] = cummulative_counts_df['cumcount'].astype('int')\n",
    "\n",
    "        return cummulative_counts_df\n",
    "\n",
    "    ###############################    \n",
    "        \n",
    "    def stats_df(self, years_of_data_per_pid):\n",
    "        historic_ehr_years_stats = years_of_data_per_pid.groupby(['ehr_domain']).agg({'person_id':'nunique', 'years_of_data'\\\n",
    "                                                                      :['mean','median', 'min','max','std']})\n",
    "\n",
    "        historic_ehr_years_stats.columns = [c[1] for c in historic_ehr_years_stats.columns]\n",
    "        historic_ehr_years_stats[['mean','median','min','max', 'std']] = \\\n",
    "        historic_ehr_years_stats[['mean','median','min','max', 'std']].apply(lambda x: round(x,2)).astype('float64')\n",
    "\n",
    "        historic_ehr_years_stats[['nunique','mean','median','min','max', 'std']] = \\\n",
    "                            format_numbers(historic_ehr_years_stats[['nunique','mean','median','min','max', 'std']])\n",
    "        historic_ehr_years_stats['min'] = historic_ehr_years_stats['min'].replace('0%','0')\n",
    "\n",
    "        name = 'Years of EHR Data Available'\n",
    "        historic_ehr_years_stats.columns = pd.MultiIndex.from_tuples(\n",
    "                                                    [('','N Participants')\n",
    "                                                    ,(f\"{name}\",'Mean')\n",
    "                                                    ,(f\"{name}\",'Median')\n",
    "                                                    ,(f\"{name}\",'Minimum')\n",
    "                                                    ,(f\"{name}\",'Maximun')\n",
    "                                                    ,(f\"{name}\",'Standard Deviation')\n",
    "                                                    ])\n",
    "\n",
    "        return style_df(historic_ehr_years_stats)\n",
    "\n",
    "    def boxplot(self, years_of_data_per_pid, w = 16, l = 8):\n",
    "\n",
    "        df_plot = years_of_data_per_pid.groupby(['ehr_domain','years_of_data'], as_index = False).nunique()\n",
    "        current_cdr = dataset.split('.')[1]       \n",
    " \n",
    "        sns.set(rc={'figure.figsize':(w, l)})\n",
    "        g = sns.boxplot(data = df_plot\n",
    "                            , x = 'ehr_domain', y = 'years_of_data', hue = 'ehr_domain'\n",
    "                            , medianprops={\"color\": \"red\"}, meanprops={\"color\": \"coral\"}\n",
    "                            , showmeans = True, dodge=False)\n",
    "\n",
    "        g.get_legend().remove()\n",
    "\n",
    "        plt.suptitle(f'''\\nIn the ({current_cdr}) for N Participants = {'{:,}'.format(self.denominator)}\\n'''\n",
    "                         , size = 16,style='italic')\n",
    "        plt.title('\\nYears of EHR Data Available\\n\\n\\n', size = 20)\n",
    "        plt.xlabel('EHR Domains', size = 16)\n",
    "        plt.ylabel('N Years of Data', size = 16)\n",
    "        plt.xticks(size = 14)\n",
    "        plt.yticks(size = 14)\n",
    "\n",
    "        plt.show()                \n",
    "    \n",
    "    ############################# 2. Historic EHR data by Domains over time, plot #############################            \n",
    "    def lineplot(self, cdr_version, add_perc_text = 'yes', rotate_x = 0, plot_start_year = 2010#, save_plot = 'yes'\n",
    "                , w = 18, l = 14):\n",
    " \n",
    "        #Transform\n",
    "        historic_ehr_cum = self.historic_ehr_df.copy()\n",
    "        historic_ehr_cum['start_year'] = [i.year for i in historic_ehr_cum['start_date']]\n",
    "        historic_ehr_cum = self.pid_cumcount_by_vars(df  = historic_ehr_cum[['person_id','ehr_domain','start_year']].drop_duplicates()\n",
    "                                                , cum_count_by = ['ehr_domain','start_year'])\n",
    "        historic_ehr_cum['cum%'] = round(historic_ehr_cum['cumcount']*100/self.denominator)\n",
    "        historic_ehr_cum['cum%'] = [int(i) for i in historic_ehr_cum['cum%']] #new added 05/14/2024\n",
    "        historic_ehr_cum = historic_ehr_cum[historic_ehr_cum['cum%'] > 0] #new added 12/7/2023\n",
    "\n",
    "        historic_ehr_cum_afterYEAR = historic_ehr_cum.copy()\n",
    "        historic_ehr_cum_afterYEAR = historic_ehr_cum_afterYEAR[historic_ehr_cum_afterYEAR.start_year >= plot_start_year]\n",
    "\n",
    "        df_plot = historic_ehr_cum_afterYEAR.drop(['cumcount'],1)\\\n",
    "                    .pivot(index = ['start_year'], columns = ['ehr_domain']).fillna(0)#.reset_index()\n",
    "        \n",
    "\n",
    "        df_plot.columns = [c[1] for c in df_plot.columns]\n",
    "\n",
    "        #current_cdr = self.dataset.split('.')[1]\n",
    "#         fig_caption = f'''\\n\\nThis plot represents EHR data availability in the current CDR ({current_cdr}). \n",
    "#         The denominator for the percentages is the Total Number of Participants in the current CDR, N = {'{:,}'.format(self.denominator)}.\n",
    "#         For better visibility, the plot only displays EHR data growth starting in 2010. '''\n",
    "        fig_caption = f'''\n",
    "\\n\\n\\nThe denominator for the percentages is the Total Number of Participants, N = {'{:,}'.format(self.denominator)}.\n",
    "For better visibility, the plot only displays EHR data growth starting in {plot_start_year}.\n",
    "Dataset version: {cdr_version}'''\n",
    "    \n",
    "        #plot\n",
    "        plt.figure(figsize=(w,l), tight_layout=True, facecolor=\"w\")       \n",
    "        plt.plot(df_plot, 'o-', linewidth=2)\n",
    "        \n",
    "\n",
    "        plt.xticks(df_plot.index, size = 13, rotation = rotate_x)\n",
    "        plt.yticks(range(int(historic_ehr_cum_afterYEAR['cum%'].min())\n",
    "                         , int(historic_ehr_cum_afterYEAR['cum%'].max()), 5),size = 13)\n",
    "        plt.gca().yaxis.set_major_formatter(PercentFormatter(decimals=0))\n",
    "        plt.xlabel('Year of EHR Data\\n\\n\\n\\n', size = 16)\n",
    "        plt.ylabel('% of Participants with EHR data', size = 16)\n",
    "        plt.title('\\nHistorical Availability of EHR Data', size = 21)\n",
    "        plt.xticks(size = 14, rotation=45)\n",
    "        plt.yticks(size = 14)\n",
    "        plt.legend(title='EHR Domains', title_fontsize = 14, labels=df_plot.columns, fontsize = 14)\n",
    "        plt.text(statistics.median(df_plot.index), -16\n",
    "             , fig_caption, verticalalignment='bottom',style='italic'\n",
    "             , horizontalalignment='center', color = 'black', size = 14)\n",
    "\n",
    "        if add_perc_text.lower() == 'yes':\n",
    "            min_year = df_plot.index.min()\n",
    "            max_year = df_plot.index.max()\n",
    "\n",
    "            for d in ['Any EHR']:\n",
    "                plt.text(min_year+0.5, df_plot[d][min_year]+2\n",
    "                         , d+' ('+str(int(df_plot[d][min_year]))+'%)',verticalalignment='center'\n",
    "                         ,horizontalalignment='right', color = 'black', size = 12, rotation = 13)\n",
    "                plt.text(max_year-0.0001, df_plot[d][max_year]+1\n",
    "                         , d+' ('+str(int(df_plot[d][max_year]))+'%)',verticalalignment='bottom'\n",
    "                         ,horizontalalignment='center', color = 'black', size = 12)\n",
    "\n",
    "            for d in ['Condition', 'Device', 'Drug'#,'Measurement'\n",
    "                      ,'Observation'#,'Procedure'\n",
    "                      ,'Visit']:\n",
    "                plt.text(min_year-0.1, df_plot[d][min_year]-0.6\n",
    "                         , str(int(df_plot[d][min_year]))+'%',verticalalignment='center'\n",
    "                         ,horizontalalignment='right', color = 'black', size = 12, rotation = 13)\n",
    "\n",
    "            for d in ['Condition', 'Device', 'Drug'#,'Measurement'\n",
    "                      ,'Observation'#,'Procedure','Visit'\n",
    "                     ]:\n",
    "                plt.text(max_year+0.3, df_plot[d][max_year]-1\n",
    "                         , str(int(df_plot[d][max_year]))+'%',verticalalignment='bottom'\n",
    "                         ,horizontalalignment='center', color = 'black', size = 12)\n",
    "    \n",
    "        plt.savefig(f'ehr_historical_availability_from_{plot_start_year}_.jpeg')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12c6eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = historic_ehr()\n",
    "historic_ehr_df = h.historic_ehr_data()\n",
    "historic_ehr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a0baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator = v7_10k_mapped.person_id.nunique()\n",
    "\n",
    "hs = historic_ehr_summary(denominator, historic_ehr_df)\n",
    "hs.lineplot(cdr_version = 'V7 Controlled Tier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbe23f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "548.8px",
    "left": "76px",
    "top": "133.525px",
    "width": "307.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
