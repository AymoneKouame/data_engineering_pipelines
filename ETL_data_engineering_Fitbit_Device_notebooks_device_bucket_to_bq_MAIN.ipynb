{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filename format example**\n",
    "\n",
    "ptsc-health-data-prod/raw/health/2022/06/30/FITBIT/DEVICE_DETAILS/DEVICE/DEVICE/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules and Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T16:23:03.512580Z",
     "start_time": "2023-04-05T16:23:03.508577Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"Your application has authenticated using end user credentials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T13:59:16.695706Z",
     "start_time": "2023-04-18T13:59:13.757541Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from calendar import monthrange\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "client = bigquery.Client(project='aou-res-curation-prod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T16:51:49.178697Z",
     "start_time": "2023-04-05T16:51:49.175686Z"
    }
   },
   "outputs": [],
   "source": [
    "device_uris = \"gs://ptsc-health-data-prod/raw/health/{yyyy}/{mm}/{dd}/FITBIT/DEVICE_DETAILS/DEVICE/DEVICE/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T17:29:40.100422Z",
     "start_time": "2023-04-05T17:29:40.095421Z"
    }
   },
   "outputs": [],
   "source": [
    "yesterday1 = datetime.today() - timedelta(days=1)\n",
    "\n",
    "#yesterday = str(yesterday1.date())\n",
    "yesterday_year = str(yesterday1.year)\n",
    "yesterday_month = str(yesterday1.month)\n",
    "yesterday_day = str(yesterday1.day)\n",
    "yesterday_year, yesterday_month, yesterday_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T13:59:19.182495Z",
     "start_time": "2023-04-18T13:59:19.174495Z"
    }
   },
   "outputs": [],
   "source": [
    "cur_project_id = 'aou-res-curation-prod'\n",
    "dataset_name = 'fitbit_ingest' #dataset_name = 'fitbit_ingest'\n",
    "main_table = 'device'\n",
    "\n",
    "print(main_table)\n",
    "\n",
    "federated_table_name = f'dev_{main_table}'\n",
    "print('\\n'+federated_table_name)\n",
    "\n",
    "federated_table_id = f\"{cur_project_id}.{dataset_name}.dev_{main_table}\"\n",
    "print('\\n'+federated_table_id)\n",
    "\n",
    "staging_table_name = f\"staging_{main_table}\"\n",
    "print('\\n'+staging_table_name)\n",
    "\n",
    "staging_table_id = f\"{cur_project_id}.{dataset_name}.{staging_table_name}\"\n",
    "print('\\n'+staging_table_id)\n",
    "\n",
    "destination_table_id = f\"{cur_project_id}.{dataset_name}.{main_table}\"\n",
    "print('\\n'+destination_table_id)\n",
    "#print(f\"Uploading {cat} to **{staging_table_id}**\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T16:23:17.467804Z",
     "start_time": "2023-04-05T16:23:17.461806Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_federated_table(dataset_name, federated_table_name, uris, project_id=cur_project_id):\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    table_ref = bq_client.dataset(dataset_name).table(federated_table_name)\n",
    "    table = bigquery.Table(table_ref)\n",
    "    extconfig = bigquery.ExternalConfig('CSV')\n",
    "    extconfig.schema = [bigquery.SchemaField('data', 'string')]\n",
    "    extconfig.options.autodetect = False\n",
    "    extconfig.options.field_delimiter = '|' #u'\\u00ff'\n",
    "    extconfig.options.allow_jagged_rows = True\n",
    "    extconfig.options.ignore_unknown_values = True\n",
    "    extconfig.options.allow_quoted_newlines = True\n",
    "    extconfig.max_bad_records = 0\n",
    "    extconfig.source_uris = uris\n",
    "    table.external_data_configuration = extconfig\n",
    "    bq_client.delete_table(table, not_found_ok=True) \n",
    "    bq_client.create_table(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T16:23:17.654816Z",
     "start_time": "2023-04-05T16:23:17.651814Z"
    }
   },
   "outputs": [],
   "source": [
    "def delete_table(table, project_id = cur_project_id, dataset_name = dataset_name):\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    table_ref = bq_client.dataset(dataset_name).table(table)\n",
    "    table = bigquery.Table(table_ref)\n",
    "    bq_client.delete_table(table, not_found_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T16:23:17.859825Z",
     "start_time": "2023-04-05T16:23:17.851824Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_table_to_bigquery(staging_table_id, destination_table_id, project_id = cur_project_id):\n",
    "    \n",
    "    # Construct a BigQuery client object.\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig( \n",
    "                schema=[\n",
    "                    bigquery.SchemaField(\"vibrent_id\", bigquery.enums.SqlTypeNames.INTEGER),\n",
    "                    bigquery.SchemaField(\"device_id\", bigquery.enums.SqlTypeNames.STRING),\n",
    "                    bigquery.SchemaField(\"upload_date\", bigquery.enums.SqlTypeNames.DATE),\n",
    "                    bigquery.SchemaField(\"date\", bigquery.enums.SqlTypeNames.DATE),\n",
    "                    bigquery.SchemaField(\"battery\", bigquery.enums.SqlTypeNames.STRING),\n",
    "                    bigquery.SchemaField(\"battery_level\", bigquery.enums.SqlTypeNames.FLOAT),\n",
    "                    bigquery.SchemaField(\"device_version\", bigquery.enums.SqlTypeNames.STRING),\n",
    "                    bigquery.SchemaField(\"device_type\", bigquery.enums.SqlTypeNames.STRING),\n",
    "                    bigquery.SchemaField(\"last_sync_time\", bigquery.enums.SqlTypeNames.DATETIME)\n",
    "                    ]\n",
    "        \n",
    "                 , write_disposition=\"WRITE_APPEND\")\n",
    "\n",
    "    # does not include the fields 'features' and 'mac' \n",
    "    #- they are not in the agred upon schema and they are either empty or not useful\n",
    "    sql_query = f\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        SAFE_CAST(REGEXP_EXTRACT(filename, 'DEVICE/DEVICE/([0-9]+)/') AS INT64) AS vibrent_id,\n",
    "        SAFE_CAST(JSON_EXTRACT_SCALAR(unnested_data, \"$.id\") AS STRING) as device_id,\n",
    "        DATE(SAFE_CAST(REPLACE(REGEXP_EXTRACT(filename, 'health/([0-9]{{4}}/[0-9]{{2}}/[0-9]{{2}})/')\n",
    "                                            , '/', '-') AS DATE)) as upload_date,    \n",
    "        SAFE_CAST(REGEXP_EXTRACT(filename, '/([0-9]{{4}}-[0-9]{{2}}-[0-9]{{2}})[T|-]?') AS DATE) as date,\n",
    "        SAFE_CAST(JSON_EXTRACT_SCALAR(unnested_data, \"$.battery\") AS STRING) as battery,\n",
    "        SAFE_CAST(JSON_EXTRACT_SCALAR(unnested_data, \"$.batteryLevel\") AS FLOAT64) as battery_level,\n",
    "        SAFE_CAST(JSON_EXTRACT_SCALAR(unnested_data, \"$.deviceVersion\") AS STRING) as device_version,\n",
    "        SAFE_CAST(JSON_EXTRACT_SCALAR(unnested_data, \"$.lastSyncTime\") AS DATETIME) as last_sync_time,\n",
    "        SAFE_CAST(JSON_EXTRACT_SCALAR(unnested_data, \"$.type\") AS STRING) as device_type,\n",
    "        \n",
    "        #SAFE_CAST(JSON_EXTRACT_SCALAR(unnested_data, \"$.features\") AS STRING) as features,\n",
    "        #SAFE_CAST(JSON_EXTRACT_SCALAR(unnested_data, \"$.mac\") AS STRING) as mac,\n",
    "    \n",
    "    FROM `{staging_table_id}` \n",
    "    , UNNEST(JSON_EXTRACT_ARRAY(data, \"$.\")) AS unnested_data \"\"\"\n",
    "\n",
    "    # Start the query, passing in the extra configuration.\n",
    "    dataframe = pd.read_gbq(sql_query)\n",
    "    dataframe['upload_date'] = [datetime.date(i) for i in dataframe['upload_date']]\n",
    "    job = client.load_table_from_dataframe(dataframe, destination_table_id, job_config = job_config)  # Make an API request.\n",
    "    return job.result()  # Wait for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T16:23:18.994882Z",
     "start_time": "2023-04-05T16:23:18.988896Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_and_parse_table_to_bigquery(federated_table_id, staging_table_id, destination_table_id\n",
    "                                     , project_id = cur_project_id):\n",
    "    # combibation of load_federated_table_to_bigquery() and parse_table_to_bigquery() \n",
    "    \n",
    "    # 1. load_federated_table_to_bigquery \n",
    "    ## Construct a BigQuery client object.\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=[\n",
    "            bigquery.SchemaField(\"data\", bigquery.enums.SqlTypeNames.STRING),\n",
    "            bigquery.SchemaField(\"filename\", bigquery.enums.SqlTypeNames.STRING)]\n",
    "        , write_disposition=\"WRITE_TRUNCATE\",\n",
    "            )\n",
    "\n",
    "    sql = f\"\"\"\n",
    "        SELECT\n",
    "            *,\n",
    "            _FILE_NAME as filename\n",
    "        FROM `{federated_table_id}`\n",
    "    \"\"\"\n",
    "    # Start the query, passing in the extra configuration.\n",
    "    dataframe = pd.read_gbq(sql)\n",
    "    if dataframe.empty == False: #avoids error message when there is no json file for that upload date\n",
    "        \n",
    "        job = client.load_table_from_dataframe(dataframe, staging_table_id, job_config = job_config)  # Make an API request.\n",
    "        res = job.result() # Make an API request.\n",
    "        \n",
    "        ## 2 - If there is data, parse_table_to_bigquery\n",
    "        parse_table_to_bigquery(staging_table_id, destination_table_id, project_id = cur_project_id)\n",
    "        \n",
    "    else:\n",
    "        res = 'no data'\n",
    "        print(res)\n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T18:23:59.347752Z",
     "start_time": "2023-04-05T18:23:59.336751Z"
    }
   },
   "outputs": [],
   "source": [
    "months_of_year = ['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "\n",
    "def daysToLoad(yyyy, mm, day_range = None):\n",
    "    if day_range == None:\n",
    "        days_to_load = range(1, monthrange(int(yyyy), int(mm))[1]+1)\n",
    "    else:\n",
    "        days_to_load = day_range\n",
    "    return days_to_load\n",
    "\n",
    "\n",
    "def run_etl(yyyy\n",
    "            , day_range = None\n",
    "            , months_to_load = months_of_year\n",
    "            , uri = device_uris\n",
    "            , dataset_name = dataset_name\n",
    "            , federated_table_name = federated_table_name\n",
    "            , federated_table_id = federated_table_id\n",
    "            , staging_table_id = staging_table_id\n",
    "            , destination_table_id = destination_table_id):\n",
    "    \n",
    "    \n",
    "    print('Uploading year', yyyy)\n",
    "    if yyyy < yesterday_year:\n",
    "        for mm in months_to_load:\n",
    "            print(\" Uploading month\", mm)\n",
    "            days_to_load = daysToLoad(yyyy, mm, day_range = day_range)\n",
    "            for day in days_to_load:\n",
    "                dd = str(day).zfill(2)\n",
    "                print(\"  Uploading day\", dd)\n",
    "                uris = uri.format(yyyy=yyyy, mm=mm, dd=dd)\n",
    "                create_federated_table(dataset_name, federated_table_name, uris)\n",
    "                \n",
    "                #delete_table(table = staging_table_name)\n",
    "                load_and_parse_table_to_bigquery(federated_table_id = federated_table_id\n",
    "                                                 , staging_table_id = staging_table_id\n",
    "                                                 , destination_table_id = destination_table_id)\n",
    "                \n",
    "                delete_table(table = staging_table_name)\n",
    "    \n",
    "    else:\n",
    "        if yyyy == yesterday_year:\n",
    "            \n",
    "            # this section ensures that data is upload up until yesterday\n",
    "            for mm in months_to_load:\n",
    "                print(\" Uploading month\", mm)\n",
    "                days_to_load = daysToLoad(yyyy, mm, day_range = day_range)                \n",
    "                if int(mm) < int(yesterday_month):\n",
    "                    days_to_load = days_to_load\n",
    "                else:\n",
    "                    if int(mm) == int(yesterday_month):\n",
    "                        days_to_load = [d for d in days_to_load if d <= int(yesterday_day)]\n",
    "                        \n",
    "                for day in days_to_load:\n",
    "                    dd = str(day).zfill(2)\n",
    "                    print(\"  Uploading day\", dd)\n",
    "                    uris = uri.format(yyyy=yyyy, mm=mm, dd=dd)\n",
    "                    create_federated_table(dataset_name, federated_table_name, uris)\n",
    "                    \n",
    "                    load_and_parse_table_to_bigquery(federated_table_id = federated_table_id\n",
    "                                                 , staging_table_id = staging_table_id\n",
    "                                                 , destination_table_id = destination_table_id)               \n",
    "                     \n",
    "                    delete_table(table = staging_table_name)\n",
    "                    \n",
    "    print('Done.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-17T18:08:45.623283Z",
     "start_time": "2023-02-17T18:08:45.619280Z"
    }
   },
   "source": [
    "*The default is for the ETL to run for all months of the year (up until yesterday's date) and all days of each month --  unless you specify otherwise as shown below*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXAMPLES**\n",
    "\n",
    "- To upload data for the whole year 2022\n",
    "\n",
    "`run_etl(yyyy = '2022')`\n",
    "\n",
    "\n",
    "- To upload data starting from May in 2022\n",
    "\n",
    "`run_etl(yyyy = '2022', months_to_load = [05','06','07','08','09','10','11','12'])`\n",
    "\n",
    "\n",
    "\n",
    "- To upload data for starting from day 17 only in June in year 2022\n",
    "\n",
    "`run_etl(yyyy = '2022', months_to_load = ['06'], day_range = range(17, 32))` **always add one to the range end day**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Triple check the dataset names, project names etc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T16:46:36.214184Z",
     "start_time": "2023-04-05T16:46:36.210182Z"
    }
   },
   "outputs": [],
   "source": [
    "print(main_table)\n",
    "print('\\ndataset_name: '+dataset_name)\n",
    "print('\\nfederated_table_name:'+federated_table_name)\n",
    "print('\\nfederated_table_id: '+federated_table_id)\n",
    "print('\\nstaging_table_name: '+staging_table_name)\n",
    "print('\\nstaging_table_id: '+staging_table_id)\n",
    "print('\\ndestination_table_id: '+destination_table_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the max upload date per dat data type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T17:09:29.644025Z",
     "start_time": "2023-04-05T17:09:27.391515Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.read_gbq('''SELECT MAX(upload_date), \n",
    "               FROM `aou-res-curation-prod.fitbit_ingest.device` ''')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*need to upfate from 2023/02/17 to 2023/04/04 (from max upload date +1 day to yesterday*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T18:45:27.217126Z",
     "start_time": "2023-04-05T18:30:33.046408Z"
    }
   },
   "outputs": [],
   "source": [
    "# UPDATE on 4/5/2023\n",
    "\n",
    "## run 1 - DONE\n",
    "#run_etl(yyyy = '2023', months_to_load = ['02'], day_range = range(17, 32))\n",
    "\n",
    "## run 2 - DONE\n",
    "#run_etl(yyyy = '2023', months_to_load = ['03', '04'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-17T20:23:20.006393Z",
     "start_time": "2023-02-17T20:09:35.291081Z"
    }
   },
   "outputs": [],
   "source": [
    "## DONE! 2.17.2023\n",
    "#run_etl(yyyy = '2023')\n",
    "\n",
    "## DONE! 2.17.2023\n",
    "#run_etl(yyyy = '2022', months_to_load = ['06','07','08','09','10','11','12'])\n",
    "\n",
    "## DONE! 2.17.2023\n",
    "#run_etl(yyyy = '2022', months_to_load = ['05'], day_range = range(17, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T18:45:42.518334Z",
     "start_time": "2023-04-05T18:45:40.112970Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check max upload date again to make sure everything updated\n",
    "pd.read_gbq('''SELECT MAX(upload_date), \n",
    "               FROM `aou-res-curation-prod.fitbit_ingest.device` ''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN/Update `fitbit_ingest.id_mapping`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T14:00:50.112885Z",
     "start_time": "2023-04-18T14:00:49.576396Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "from config import connect_options\n",
    "\n",
    "#cloud_sql_proxy_x64 -dir=./ -instances=all-of-us-rdr-prod:us-central1:rdrbackupdb-e=tcp:3308\n",
    "con = mysql.connector.connect(**connect_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T14:01:00.734866Z",
     "start_time": "2023-04-18T14:00:51.993257Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "     SELECT DISTINCT\n",
    "        participant_id,\n",
    "        CAST(external_id AS UNSIGNED) as vibrent_id\n",
    "    FROM rdr.participant\n",
    "    WHERE external_id IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "rdr_pids = pd.read_sql_query(query, con=con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T14:01:11.749059Z",
     "start_time": "2023-04-18T14:01:00.736864Z"
    }
   },
   "outputs": [],
   "source": [
    "project_id = \"aou-res-curation-prod\"\n",
    "destination_table=\"fitbit_ingest.id_mapping\"\n",
    "\n",
    "rdr_pids.to_gbq(destination_table, project_id, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device Data Update History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***- Aymone updated on 2.17.2023: from beginning of data (05/17/2022 to 2/16/2023)***\n",
    "\n",
    "    - year 2020 and 2021 - no device data\n",
    "\n",
    "    - year 2022: from 05/17/2022 to 12/31/2022\n",
    "\n",
    "    - Year 2023: from 01/01 to 2/16/2023\n",
    "\n",
    "***- Aymone updated on 4.05.2023: from 2023/02/17 to 2023/04/04***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T18:44:33.120327Z",
     "start_time": "2023-02-14T18:44:33.118326Z"
    }
   },
   "source": [
    "# QC ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **See notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating View `v_device` - RUN ONLY WHEN ALL IS DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T14:18:39.445615Z",
     "start_time": "2023-04-18T14:18:39.441613Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_view(view_query, destination_view_name, dataset_name = dataset_name, project_id = cur_project_id):\n",
    "\n",
    "    client = bigquery.Client(project_id)\n",
    "\n",
    "    query_job = client.query(view_query.format(project_id = project_id\n",
    "                                               , dataset_name = dataset_name\n",
    "                                               , destination_view_name = destination_view_name))  # Make an API request.\n",
    "    query_job.result()  # Wait for the job to complete.    print(f\"Created {view.table_type}: {str(view.reference)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T14:18:40.489055Z",
     "start_time": "2023-04-18T14:18:40.486083Z"
    }
   },
   "outputs": [],
   "source": [
    "v_device_query = '''\n",
    "    CREATE OR REPLACE VIEW `{project_id}.{dataset_name}.{destination_view_name}`\n",
    "    \n",
    "    AS (SELECT * EXCEPT (rn, upload_date)\n",
    "        FROM (SELECT \n",
    "                m.participant_id AS person_id, \n",
    "                d.* EXCEPT (vibrent_id),\n",
    "                ROW_NUMBER() OVER(PARTITION BY vibrent_id, device_id, date\n",
    "                , battery , device_version, device_type, last_sync_time\n",
    "                ORDER BY upload_date DESC) AS rn\n",
    "             FROM `aou-res-curation-prod.fitbit_ingest.device` d\n",
    "             JOIN `aou-res-curation-prod.fitbit_ingest.id_mapping` m USING(vibrent_id)\n",
    "            )\n",
    "        WHERE rn = 1\n",
    "        )\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T14:18:43.224375Z",
     "start_time": "2023-04-18T14:18:41.141194Z"
    }
   },
   "outputs": [],
   "source": [
    "create_view(v_device_query, 'v_device')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T13:59:48.445185Z",
     "start_time": "2023-04-18T13:59:34.089125Z"
    }
   },
   "outputs": [],
   "source": [
    "## copy a table from apple_healthkit_ingest to test_apple_healthkit_ingest\n",
    "def copy_table(origin_table_name, destination_table_name, disposition = 'WRITE_TRUNCATE'\n",
    "                           , dataset_name = dataset_name, project_id = cur_project_id):\n",
    "\n",
    "    \n",
    "    query = '''SELECT * FROM `{project_id}.test_fitbit_ingest.{origin_table_name}`'''\n",
    "    \n",
    "    client = bigquery.Client(project_id)\n",
    "    job_config = bigquery.QueryJobConfig(\n",
    "                        allow_large_results=True\n",
    "                        , destination=f'{project_id}.{dataset_name}.{destination_table_name}'\n",
    "                        , write_disposition=disposition\n",
    "                        #, use_legacy_sql=True\n",
    "    )\n",
    "        \n",
    "    query_job = client.query(query.format(project_id= project_id, origin_table_name = origin_table_name)\n",
    "                             , job_config=job_config)  # Make an API request.\n",
    "    query_job.result()  # Wait for the job to complete.\n",
    "    \n",
    "copy_table(origin_table_name = 'device', destination_table_name = 'device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T17:12:50.346143Z",
     "start_time": "2023-04-05T17:12:50.334143Z"
    }
   },
   "outputs": [],
   "source": [
    "# RECYCLED\n",
    "# months_of_year = ['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "\n",
    "# def daysToLoad(yyyy, mm, day_range = None):\n",
    "#     if day_range == None:\n",
    "#         days_to_load = range(1, monthrange(int(yyyy), int(mm))[1]+1)\n",
    "#     else:\n",
    "#         days_to_load = day_range\n",
    "#     return days_to_load\n",
    "\n",
    "\n",
    "# def run_etl(yyyy\n",
    "#             , day_range = None\n",
    "#             , months_to_load = months_of_year\n",
    "#             , uri = device_uris\n",
    "#             , dataset_name = dataset_name\n",
    "#             , federated_table_name = federated_table_name\n",
    "#             , federated_table_id = federated_table_id\n",
    "#             , staging_table_id = staging_table_id\n",
    "#             , destination_table_id = destination_table_id):\n",
    "    \n",
    "    \n",
    "#     print('Uploading year', yyyy)\n",
    "#     if yyyy < yesterday_year:\n",
    "#         for mm in months_to_load:\n",
    "#             print(\" Uploading month\", mm)\n",
    "#             days_to_load = daysToLoad(yyyy, mm, day_range = day_range)\n",
    "#             for day in days_to_load:\n",
    "#                 dd = str(day).zfill(2)\n",
    "#                 print(\"  Uploading day\", dd)\n",
    "#                 uris = uri.format(yyyy=yyyy, mm=mm, dd=dd)\n",
    "#                 create_federated_table(dataset_name, federated_table_name, uris)\n",
    "                \n",
    "#                 #delete_table(table = staging_table_name)\n",
    "#                 load_and_parse_table_to_bigquery(federated_table_id = federated_table_id\n",
    "#                                                  , staging_table_id = staging_table_id\n",
    "#                                                  , destination_table_id = destination_table_id)\n",
    "                \n",
    "#                 delete_table(table = staging_table_name)\n",
    "    \n",
    "#     else:\n",
    "#         if yyyy == yesterday_year:\n",
    "#             # this section ensures that data is upload up until yesterday\n",
    "#             for mm in [m for m in months_to_load if int(m) <= int(yesterday_month)]:\n",
    "#                 print(\" Uploading month\", mm)\n",
    "#                 days_to_load = daysToLoad(yyyy, mm, day_range = day_range)\n",
    "#                 days_to_load = [d for d in days_to_load if d <= int(yesterday_day)]\n",
    "#                 for day in days_to_load:\n",
    "#                     dd = str(day).zfill(2)\n",
    "#                     if dd <= \"00\":\n",
    "#                         continue\n",
    "#                     if dd > yesterday_day:\n",
    "#                         break\n",
    "#                     print(\"  Uploading day\", dd)\n",
    "#                     uris = uri.format(yyyy=yyyy, mm=mm, dd=dd)\n",
    "#                     create_federated_table(dataset_name, federated_table_name, uris)\n",
    "                    \n",
    "#                     load_and_parse_table_to_bigquery(federated_table_id = federated_table_id\n",
    "#                                                  , staging_table_id = staging_table_id\n",
    "#                                                  , destination_table_id = destination_table_id)               \n",
    "                     \n",
    "#                     delete_table(table = staging_table_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
