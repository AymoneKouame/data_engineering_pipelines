{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a federated table in bigquery\n",
    "\n",
    "- Create a BigQuery federated table that connects to and directly reads from the RDR bucket\n",
    "    \n",
    "    - Fitbit\n",
    "    \n",
    "        - DAILY_ACTIVITY_SUMMARY: /NA/ACTIVITY_MEASUREMENTS\n",
    "        - DAILY_FAT\n",
    "        - DAILY_SLEEP_SUMMARY\n",
    "        - DAILY_WEIGHT\n",
    "        - FOOD\n",
    "        - HEARTRATE\n",
    "        - INTRADAY_STEPS\n",
    "        - WATER\n",
    "       \n",
    "    - Apple Healthkit\n",
    "    \n",
    "    \n",
    "- To create a table with the filename, create another empty table and change options to save query in that newly created table, run query\n",
    "\n",
    "```\n",
    "SELECT\n",
    "*,\n",
    "_FILE_NAME as filename\n",
    "FROM\n",
    "`R2019q4r3_deid_io_f.test3`\n",
    "order by 1, 2\n",
    "```\n",
    "\n",
    "- or load into big query using `bq load --source_format=CSV -F '^' R2019q4r3_deid_io_f.test2 gs://ptsc-fitbit-data-all-of-us-rdr-prod/HEARTRATE/*.json`\n",
    "\n",
    "- To delete: `run bq rm -f -t R2019q4r3_deid_io_f.test3` in cloud shell terminal\n",
    "\n",
    "- parse raw into minute level data by running\n",
    "```\n",
    "SELECT\n",
    " CAST(JSON_EXTRACT_SCALAR(data,\"$.activities-heart[0]['dateTime']\") AS DATE) AS day,\n",
    " JSON_EXTRACT_SCALAR(params ,\"$.time\") as time,\n",
    " JSON_EXTRACT_SCALAR(params,\"$.value\") as heart_rate,\n",
    "REGEXP_EXTRACT(filename, '/([0-9]+)/') as userid\n",
    "FROM\n",
    "`R2019q4r3_deid_io_f.heart_raw`, unnest(JSON_EXTRACT_ARRAY(data,\"$.activities-heart-intraday['dataset']\")) as params\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory structure\n",
    "\n",
    "`source/datatype/payloadtype/category/YYYY/MM/DD/pid/startdate_enddate-uuid.[ext]`\n",
    "\n",
    "- ptsc-health-data-prod/raw/health/2020/03/15/FITBIT/DAILY_ACTIVITY_SUMMARY/NA/ACTIVITY_MEASUREMENTS\n",
    "- ptsc-health-data-prod/raw/health/2020/03/15/FITBIT/DAILY_FAT/NA/PHYSICAL_MEASUREMENTS\n",
    "- ptsc-health-data-prod/raw/health/2020/03/15/FITBIT/DAILY_SLEEP_SUMMARY/NA/ACTIVITY_MEASUREMENTS\n",
    "- ptsc-health-data-prod/raw/health/2020/03/15/FITBIT/DAILY_WEIGHT/NA/PHYSICAL_MEASUREMENTS\n",
    "- ptsc-health-data-prod/raw/health/2020/03/15/FITBIT/FOOD/NA/NUTRITION_TRACKING\n",
    "- ptsc-health-data-prod/raw/health/2020/03/15/FITBIT/HEARTRATE/NA/VITAL_MEASUREMENTS\n",
    "- ptsc-health-data-prod/raw/health/2020/03/15/FITBIT/INTRADAY_STEPS/NA/ACTIVITY_MEASUREMENTS\n",
    "- ptsc-health-data-prod/raw/health/2020/03/15/FITBIT/WATER/NA/NUTRITION_TRACKING\n",
    "\n",
    "YYYY: 2020, 2021\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Created federated table\n",
    "\n",
    "Step 2: Create staging table with data as string + add filename\n",
    "\n",
    "Step 3: Parse the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T15:12:43.553772Z",
     "start_time": "2023-03-23T15:12:42.663714Z"
    }
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T19:13:24.995009Z",
     "start_time": "2023-02-09T19:13:24.989004Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_federated_table(dataset_name, federated_table_name, uris, project_id='aou-res-curation-prod'):\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    table_ref = bq_client.dataset(dataset_name).table(federated_table_name)\n",
    "    table = bigquery.Table(table_ref)\n",
    "    extconfig = bigquery.ExternalConfig('CSV')\n",
    "    extconfig.schema = [bigquery.SchemaField('data', 'STRING')]\n",
    "    extconfig.options.autodetect = False\n",
    "    extconfig.options.field_delimiter = '|' #u'\\u00ff'\n",
    "#     extconfig.options.quote_character = ''\n",
    "    # extconfig.compression = 'GZIP'\n",
    "    extconfig.options.allow_jagged_rows = True\n",
    "    extconfig.options.ignore_unknown_values = True\n",
    "    extconfig.options.allow_quoted_newlines = True\n",
    "#     extconfig.max_bad_records = 10000000\n",
    "    extconfig.max_bad_records = 0\n",
    "    extconfig.source_uris = uris\n",
    "    table.external_data_configuration = extconfig\n",
    "    bq_client.delete_table(table, not_found_ok=True) \n",
    "    bq_client.create_table(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T19:13:27.168906Z",
     "start_time": "2023-02-09T19:13:27.162908Z"
    }
   },
   "outputs": [],
   "source": [
    "def delete_table(project_id, dataset_name, table):\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    table_ref = bq_client.dataset(dataset_name).table(table)\n",
    "    table = bigquery.Table(table_ref)\n",
    "    bq_client.delete_table(table, not_found_ok=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T19:13:30.007498Z",
     "start_time": "2023-02-09T19:13:30.002496Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_federated_table_to_bigquery(federated_table_id, destination_table_id, project_id = 'aou-res-curation-prod'):\n",
    "    # Construct a BigQuery client object.\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    job_config = bigquery.QueryJobConfig(destination=destination_table_id, write_disposition = 'WRITE_APPEND')\n",
    "    sql = \"\"\"\n",
    "        SELECT\n",
    "            *,\n",
    "            _FILE_NAME as filename\n",
    "        FROM `{federated_table_id}`\n",
    "    \"\"\"\n",
    "    # Start the query, passing in the extra configuration.\n",
    "    query_job = client.query(sql.format(federated_table_id=federated_table_id), \n",
    "                             job_config=job_config)  # Make an API request.\n",
    "    return query_job.result()  # Wait for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T19:13:30.197517Z",
     "start_time": "2023-02-09T19:13:30.194509Z"
    }
   },
   "outputs": [],
   "source": [
    "URIS = {\n",
    "    'DAILY_ACTIVITY_SUMMARY': \"gs://ptsc-health-data-prod/raw/health/{yyyy}/{mm}/{dd}/FITBIT/DAILY_ACTIVITY_SUMMARY/NA/ACTIVITY_MEASUREMENTS/*\",\n",
    "    'HEARTRATE': \"gs://ptsc-health-data-prod/raw/health/{yyyy}/{mm}/{dd}/FITBIT/HEARTRATE/NA/VITAL_MEASUREMENTS/*\",\n",
    "    'INTRADAY_STEPS': \"gs://ptsc-health-data-prod/raw/health/{yyyy}/{mm}/{dd}/FITBIT/INTRADAY_STEPS/NA/ACTIVITY_MEASUREMENTS/*\",\n",
    "    'DAILY_SLEEP_SUMMARY': \"gs://ptsc-health-data-prod/raw/health/{yyyy}/{mm}/{dd}/FITBIT/DAILY_SLEEP_SUMMARY/NA/ACTIVITY_MEASUREMENTS/*\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cur_project_id = 'aou-res-curation-prod'\n",
    "# dataset_name = 'fitbit_ingest'\n",
    "\n",
    "# for cat in URIS:\n",
    "#     print(\"Uploading \", cat)\n",
    "#     cat_lower_case = cat.lower()\n",
    "#     federated_table_name = 'dev_' + cat_lower_case\n",
    "#     federated_table_id = f\"{cur_project_id}.{dataset_name}.dev_{cat_lower_case}\"\n",
    "#     destination_table_id = f\"{cur_project_id}.{dataset_name}.staging_{cat_lower_case}\"\n",
    "#     project_id, dataset_name, table = destination_table_id.split(\".\")\n",
    "#     delete_table(project_id, dataset_name, table)\n",
    "#     yyyy = '2022'\n",
    "#     print('Uploading year', yyyy)\n",
    "#     for mm in ['01']:\n",
    "#         print(\"Uploading month\", mm)\n",
    "#         for day in range(1, 32):\n",
    "#             dd = str(day).zfill(2)\n",
    "#             if dd <= \"30\":\n",
    "#                 continue\n",
    "#             if dd > \"31\":\n",
    "#                 break\n",
    "#             print(\"Uploading day\", dd)\n",
    "#             uris = URIS[cat].format(yyyy=yyyy, mm=mm, dd=dd)\n",
    "#             create_federated_table(dataset_name, federated_table_name, uris)\n",
    "#             load_federated_table_to_bigquery(federated_table_id, destination_table_id)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T19:17:00.673420Z",
     "start_time": "2023-02-09T19:16:54.372210Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cur_project_id = 'aou-res-curation-prod'\n",
    "dataset_name = 'fitbit_ingest'\n",
    "\n",
    "for cat in URIS:\n",
    "    print(\"Uploading \", cat)\n",
    "    cat_lower_case = cat.lower()\n",
    "    federated_table_name = 'dev_' + cat_lower_case\n",
    "    federated_table_id = f\"{cur_project_id}.{dataset_name}.dev_{cat_lower_case}\"\n",
    "    destination_table_id = f\"{cur_project_id}.{dataset_name}.staging_{cat_lower_case}\"\n",
    "    project_id, dataset_name, table = destination_table_id.split(\".\")\n",
    "    delete_table(project_id, dataset_name, table)\n",
    "    yyyy = '2022'\n",
    "    print('Uploading year', yyyy)\n",
    "    for mm in ['07']:\n",
    "        print(\"Uploading month\", mm)\n",
    "        for day in range(1, 32):\n",
    "            dd = str(day).zfill(2)\n",
    "            if dd <= \"21\":\n",
    "                continue\n",
    "            if dd > \"31\":\n",
    "                break\n",
    "            print(\"Uploading day\", dd)\n",
    "            uris = URIS[cat].format(yyyy=yyyy, mm=mm, dd=dd)\n",
    "            create_federated_table(dataset_name, federated_table_name, uris)\n",
    "            load_federated_table_to_bigquery(federated_table_id, destination_table_id)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T21:19:46.915795Z",
     "start_time": "2022-07-22T21:19:46.911777Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next:\n",
    "\n",
    "    - mm = 02, dd<=09, dd>31 -- Done\n",
    "    `Run .4 staging`\n",
    "    \n",
    "    - mm = 03, dd<=00, dd>31 -- Done\n",
    "    `Run .4 staging`\n",
    "    \n",
    "    - mm = 04, dd<=00, dd>31 -- Done\n",
    "    `Run .4 staging`\n",
    "    \n",
    "    - mm = 05, dd<=00, dd>31-- Done\n",
    "    `Run .4 staging`\n",
    "    \n",
    "    - mm = 06, dd<=00, dd>31-- Done\n",
    "    `Run .4 staging`\n",
    "    \n",
    "    - mm = 07, dd<=00, dd>21-- Done\n",
    "    `Run .4 staging`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T21:20:36.258811Z",
     "start_time": "2022-07-22T21:20:36.254807Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_table_to_bigquery(sql_query, destination_table_id, project_id):\n",
    "    # Construct a BigQuery client object.\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    job_config = bigquery.QueryJobConfig(destination= destination_table_id, \n",
    "                                         write_disposition = 'WRITE_APPEND')\n",
    "    # Start the query, passing in the extra configuration.\n",
    "    query_job = client.query(sql_query, job_config=job_config)  # Make an API request.\n",
    "    return query_job.result()  # Wait for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T21:20:36.454159Z",
     "start_time": "2022-07-22T21:20:36.447160Z"
    }
   },
   "outputs": [],
   "source": [
    "src_table_id = {\n",
    "    'activity_summary': 'staging_daily_activity_summary',\n",
    "    'heart_rate_minute_level': 'staging_heartrate',\n",
    "    'heart_rate_summary': 'staging_heartrate',\n",
    "    'steps_intraday': 'staging_intraday_steps',\n",
    "    'sleep_level': 'staging_daily_sleep_summary',\n",
    "    'sleep_daily_summary': 'staging_daily_sleep_summary'\n",
    "}\n",
    "\n",
    "queries = {\n",
    "'activity_summary': \"\"\"\n",
    "SELECT DISTINCT\n",
    "    CAST(REGEXP_EXTRACT(filename, 'ACTIVITY_MEASUREMENTS/([0-9]+)/') AS INT64) AS vibrent_id,\n",
    "    SAFE_CAST(REPLACE(REGEXP_EXTRACT(filename, 'health/([0-9]{{4}}/[0-9]{{2}}/[0-9]{{2}})/'), '/', '-') AS DATE) as upload_date,\n",
    "    SAFE_CAST(REGEXP_EXTRACT(filename, '/([0-9]{{4}}-[0-9]{{2}}-[0-9]{{2}})[T|-]?') AS DATE) as date,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data, \"$.summary['activityCalories']\") AS FLOAT64) as activity_calories,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.summary['caloriesBMR']\") AS FLOAT64) as calories_bmr,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.summary['caloriesOut']\") AS FLOAT64) as calories_out,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.summary['elevation']\") AS FLOAT64) as elevation,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.summary['fairlyActiveMinutes']\") AS FLOAT64) as fairly_active_minutes,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.summary['floors']\") AS INT64) as floors,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.summary['lightlyActiveMinutes']\") AS FLOAT64) as lightly_active_minutes,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.summary['marginalCalories']\") AS FLOAT64) as marginal_calories,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.summary['sedentaryMinutes']\") AS FLOAT64) as sedentary_minutes,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.summary['steps']\") AS INT64) as steps,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.summary['veryActiveMinutes']\") AS FLOAT64) as very_active_minutes\n",
    "FROM `{dataset}.staging_daily_activity_summary`\"\"\", \n",
    "\n",
    "'heart_rate_minute_level': \"\"\"\n",
    "SELECT DISTINCT\n",
    "    CAST(REGEXP_EXTRACT(filename, 'VITAL_MEASUREMENTS/([0-9]+)/') AS INT64) as vibrent_id,\n",
    "    SAFE_CAST(REPLACE(REGEXP_EXTRACT(filename, 'health/([0-9]{{4}}/[0-9]{{2}}/[0-9]{{2}})/'), '/', '-') AS DATE) as upload_date,\n",
    "    DATETIME(CAST(JSON_EXTRACT_SCALAR(data,\"$.activities-heart[0]['dateTime']\") as DATE)\n",
    "    , CAST(JSON_EXTRACT_SCALAR(params ,\"$.time\") AS TIME)) as datetime,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(params,\"$.value\") AS INT64) as heart_rate_value\n",
    "FROM `{dataset}.staging_heartrate`, unnest(JSON_EXTRACT_ARRAY(data,\"$.activities-heart-intraday['dataset']\")) as params\"\"\", \n",
    "\n",
    "\n",
    "'heart_rate_summary': \"\"\"\n",
    "SELECT DISTINCT\n",
    "    CAST(REGEXP_EXTRACT(filename, 'VITAL_MEASUREMENTS/([0-9]+)/') AS INT64) as vibrent_id,\n",
    "    CAST(REPLACE(REGEXP_EXTRACT(filename, 'health/([0-9]{{4}}/[0-9]{{2}}/[0-9]{{2}})/'), '/', '-') AS DATE) as upload_date,\n",
    "    CAST(JSON_EXTRACT_SCALAR(data,\"$.activities-heart[0]['dateTime']\") AS DATE) AS date,\n",
    "    JSON_EXTRACT_SCALAR(zone,\"$.name\") as zone_name,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(zone,\"$.min\") AS INT64) as min_heart_rate,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(zone,\"$.max\") AS INT64) as max_heart_rate,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(zone,\"$.minutes\") AS INT64) as minute_in_zone,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(zone,\"$.caloriesOut\") AS FLOAT64) as calorie_count\n",
    "FROM `{dataset}.staging_heartrate`, \n",
    "unnest(JSON_EXTRACT_ARRAY(data, \"$.activities-heart[0]['value']['heartRateZones']\")) as zone\"\"\",\n",
    "\n",
    "\n",
    "'steps_intraday': \"\"\"\n",
    "SELECT DISTINCT\n",
    "    CAST(REGEXP_EXTRACT(filename, 'ACTIVITY_MEASUREMENTS/([0-9]+)/') AS INT64) AS vibrent_id,\n",
    "    CAST(REPLACE(REGEXP_EXTRACT(filename, 'health/([0-9]{{4}}/[0-9]{{2}}/[0-9]{{2}})/'), '/', '-') AS DATE) as upload_date,\n",
    "    DATETIME(CAST(JSON_EXTRACT_SCALAR(data,\"$.activities-steps[0]['dateTime']\") as DATE), CAST(JSON_EXTRACT_SCALAR(params ,\"$.time\") AS TIME)) as datetime,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(params,\"$.value\") AS NUMERIC) as steps\n",
    "FROM `{dataset}.staging_intraday_steps`, \n",
    "unnest(JSON_EXTRACT_ARRAY(data,\"$.activities-steps-intraday['dataset']\")) as params\"\"\",\n",
    "\n",
    "'sleep_level': \"\"\"\n",
    "-- first sleep data\n",
    "with first_long_data AS (\n",
    "SELECT DISTINCT\n",
    "    CAST(REGEXP_EXTRACT(filename, 'ACTIVITY_MEASUREMENTS/([0-9]+)/') AS INT64) as vibrent_id,\n",
    "    CAST(REPLACE(REGEXP_EXTRACT(filename, 'health/([0-9]{{4}}/[0-9]{{2}}/[0-9]{{2}})/'), '/', '-') AS DATE) as upload_date,\n",
    "    CAST(JSON_EXTRACT_SCALAR(data,\"$.sleep[0]['dateOfSleep']\") AS DATE) AS sleep_date,\n",
    "    JSON_EXTRACT_SCALAR(data,\"$.sleep[0]['isMainSleep']\") AS is_main_sleep,\n",
    "    CAST(JSON_EXTRACT_SCALAR(data,\"$.sleep[0]['logId']\") AS NUMERIC) AS log_id,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.summary['totalTimeInBed']\") AS INTEGER) AS minute_in_bed,\n",
    "    JSON_EXTRACT_SCALAR(level,\"$.level\") as level,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(level,\"$.dateTime\") AS DATETIME) as start_datetime,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(level,\"$.seconds\") AS FLOAT64)/60 as duration_in_min\n",
    "FROM `{dataset}.staging_daily_sleep_summary`,\n",
    "unnest(JSON_EXTRACT_ARRAY(data, \"$.sleep[0]['levels']['data']\")) as level\n",
    "),\n",
    "\n",
    "-- second sleep data\n",
    "second_long_data AS (\n",
    "SELECT DISTINCT\n",
    "    CAST(REGEXP_EXTRACT(filename, 'ACTIVITY_MEASUREMENTS/([0-9]+)/') AS INT64) as vibrent_id,\n",
    "    CAST(REPLACE(REGEXP_EXTRACT(filename, 'health/([0-9]{{4}}/[0-9]{{2}}/[0-9]{{2}})/'), '/', '-') AS DATE) as upload_date,\n",
    "    CAST(JSON_EXTRACT_SCALAR(data,\"$.sleep[1]['dateOfSleep']\") AS DATE) AS sleep_date,\n",
    "    JSON_EXTRACT_SCALAR(data,\"$.sleep[1]['isMainSleep']\") AS is_main_sleep,\n",
    "    CAST(JSON_EXTRACT_SCALAR(data,\"$.sleep[1]['logId']\") AS NUMERIC) AS log_id,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.summary['totalTimeInBed']\") AS INTEGER) AS minute_in_bed,\n",
    "    JSON_EXTRACT_SCALAR(level,\"$.level\") as level,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(level,\"$.dateTime\") AS DATETIME) as start_datetime,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(level,\"$.seconds\") AS FLOAT64)/60 as duration_in_min\n",
    "FROM `{dataset}.staging_daily_sleep_summary`,\n",
    "unnest(JSON_EXTRACT_ARRAY(data, \"$.sleep[1]['levels']['data']\")) as level\n",
    "),\n",
    "\n",
    "long_data AS (\n",
    "SELECT *\n",
    "FROM first_long_data\n",
    "UNION ALL\n",
    "SELECT *\n",
    "FROM second_long_data\n",
    ")\n",
    "\n",
    "-- remove duplicates\n",
    "SELECT * EXCEPT(rank, log_id, minute_in_bed)\n",
    "FROM (\n",
    "    SELECT * , rank() OVER(PARTITION BY vibrent_id, upload_date, sleep_date, log_id ORDER BY minute_in_bed DESC)  AS rank\n",
    "    FROM long_data\n",
    "    )\n",
    "WHERE rank = 1\n",
    "AND start_datetime IS NOT NULL\n",
    "AND sleep_date IS NOT NULL\n",
    "\"\"\",\n",
    "    \n",
    "    \n",
    "'sleep_daily_summary': \"\"\"\n",
    "WITH first_sleep_data AS (\n",
    "SELECT DISTINCT\n",
    "    CAST(REGEXP_EXTRACT(filename, 'ACTIVITY_MEASUREMENTS/([0-9]+)/') AS INT64) as vibrent_id,\n",
    "    CAST(REPLACE(REGEXP_EXTRACT(filename, 'health/([0-9]{{4}}/[0-9]{{2}}/[0-9]{{2}})/'), '/', '-') AS DATE) as upload_date,\n",
    "    CAST(JSON_EXTRACT_SCALAR(data,\"$.sleep[0]['dateOfSleep']\") AS DATE) AS sleep_date,\n",
    "    JSON_EXTRACT_SCALAR(data,\"$.sleep[0]['isMainSleep']\") AS is_main_sleep,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.sleep[0]['timeInBed']\") AS INTEGER) AS minute_in_bed,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.sleep[0]['minutesAsleep']\") AS INTEGER) AS minute_asleep,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.sleep[0]['minutesAfterWakeup']\") AS INTEGER) AS minute_after_wakeup,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.sleep[0]['minutesAwake']\") AS INTEGER) AS minute_awake,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.sleep[0]['levels']['summary']['restless']['minutes']\") AS INTEGER) AS minute_restless,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.sleep[0]['levels']['summary']['deep']['minutes']\") AS INTEGER) AS minute_deep,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.sleep[0]['levels']['summary']['light']['minutes']\") AS INTEGER) AS minute_light,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.sleep[0]['levels']['summary']['rem']['minutes']\") AS INTEGER) AS minute_rem,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.sleep[0]['levels']['summary']['wake']['minutes']\") AS INTEGER) AS minute_wake\n",
    "FROM `{dataset}.staging_daily_sleep_summary`\n",
    "),\n",
    "\n",
    "second_sleep_data AS (\n",
    "SELECT DISTINCT\n",
    "    CAST(REGEXP_EXTRACT(filename, 'ACTIVITY_MEASUREMENTS/([0-9]+)/') AS INT64) as vibrent_id,\n",
    "    CAST(REPLACE(REGEXP_EXTRACT(filename, 'health/([0-9]{{4}}/[0-9]{{2}}/[0-9]{{2}})/'), '/', '-') AS DATE) as upload_date,\n",
    "    CAST(JSON_EXTRACT_SCALAR(data,\"$.sleep[1]['dateOfSleep']\") AS DATE) AS sleep_date,\n",
    "    JSON_EXTRACT_SCALAR(data,\"$.sleep[1]['isMainSleep']\") AS is_main_sleep,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.sleep[1]['timeInBed']\") AS INTEGER) AS minute_in_bed,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.sleep[1]['minutesAsleep']\") AS INTEGER) AS minute_asleep,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.sleep[1]['minutesAfterWakeup']\") AS INTEGER) AS minute_after_wakeup,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.sleep[1]['minutesAwake']\") AS INTEGER) AS minute_awake,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.sleep[1]['levels']['summary']['restless']['minutes']\") AS INTEGER) AS minute_restless,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.sleep[1]['levels']['summary']['deep']['minutes']\") AS INTEGER) AS minute_deep,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.sleep[1]['levels']['summary']['light']['minutes']\") AS INTEGER) AS minute_light,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.sleep[1]['levels']['summary']['rem']['minutes']\") AS INTEGER) AS minute_rem,\n",
    "    SAFE_CAST(JSON_EXTRACT_SCALAR(data,\"$.sleep[1]['levels']['summary']['wake']['minutes']\") AS INTEGER) AS minute_wake\n",
    "FROM `{dataset}.staging_daily_sleep_summary`\n",
    "),\n",
    "\n",
    "long_data AS (\n",
    "    SELECT * FROM first_sleep_data\n",
    "    UNION ALL \n",
    "    SELECT * FROM second_sleep_data\n",
    ")\n",
    "\n",
    "SELECT * EXCEPT(rank)\n",
    "FROM (\n",
    "    SELECT * , RANK() OVER(PARTITION BY vibrent_id, upload_date, sleep_date, is_main_sleep ORDER BY minute_in_bed DESC) AS rank\n",
    "    FROM long_data\n",
    "    )\n",
    "WHERE rank = 1\n",
    "AND sleep_date IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T21:28:23.643133Z",
     "start_time": "2022-07-22T21:20:37.646877Z"
    }
   },
   "outputs": [],
   "source": [
    "project_id = 'aou-res-curation-prod'\n",
    "dataset_name = 'fitbit_ingest'\n",
    "\n",
    "for table_name, query in queries.items():\n",
    "    source_table_id = src_table_id[table_name]\n",
    "    sql_query = query.format(dataset=dataset_name)\n",
    "    destination_table_id = project_id + '.' + dataset_name + '.' + table_name\n",
    "    print(\"Loading\", table_name)\n",
    "    result = parse_table_to_bigquery(sql_query, destination_table_id, project_id)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T21:28:23.649129Z",
     "start_time": "2022-07-22T21:28:23.645130Z"
    }
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Vibrent ID - Participant ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T21:32:24.951709Z",
     "start_time": "2022-07-22T21:32:24.018557Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "from config import connect_options\n",
    "\n",
    "con = mysql.connector.connect(**connect_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T21:32:32.434008Z",
     "start_time": "2022-07-22T21:32:26.530102Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "     SELECT DISTINCT\n",
    "        participant_id,\n",
    "        CAST(external_id AS UNSIGNED) as vibrent_id\n",
    "    FROM rdr.participant\n",
    "    WHERE external_id IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "rdr_pids = pd.read_sql_query(query, con=con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T21:32:58.925777Z",
     "start_time": "2022-07-22T21:32:36.635992Z"
    }
   },
   "outputs": [],
   "source": [
    "project_id = \"aou-res-curation-prod\"\n",
    "destination_table=\"fitbit_ingest.id_mapping\"\n",
    "\n",
    "rdr_pids.to_gbq(destination_table, project_id, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T21:41:50.340418Z",
     "start_time": "2022-07-22T21:41:50.337414Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Lastest update to fitbit data: '+str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:03:18.993055Z",
     "start_time": "2023-03-23T16:03:18.988055Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cur_project_id = 'aou-res-curation-prod'\n",
    "dataset_name = 'fitbit_ingest'\n",
    "\n",
    "#####\n",
    "def create_view(view_query, destination_view_name, dataset_name = dataset_name, project_id = cur_project_id):\n",
    "\n",
    "    client = bigquery.Client(project_id)\n",
    "\n",
    "    query_job = client.query(view_query.format(cur_project_id = cur_project_id\n",
    "                                               , dataset_name = dataset_name\n",
    "                                               , destination_view_name = destination_view_name))  # Make an API request.\n",
    "    query_job.result()  # Wait for the job to complete.    print(f\"Created {view.table_type}: {str(view.reference)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:12:37.854058Z",
     "start_time": "2023-03-23T16:12:37.850056Z"
    }
   },
   "outputs": [],
   "source": [
    "device_view_query = ''' \n",
    "    CREATE VIEW `{cur_project_id}.{dataset_name}.{destination_view_name}`\n",
    "    AS (SELECT * EXCEPT (rn, upload_date)\n",
    "        FROM ( SELECT m.participant_id AS person_id, \n",
    "                t.* EXCEPT (vibrent_id),\n",
    "                ROW_NUMBER() OVER(PARTITION BY vibrent_id, date ORDER BY upload_date DESC) AS rn\n",
    "                FROM `{dataset_name}.device` t\n",
    "                JOIN `{dataset_name}.id_mapping` m USING(vibrent_id)\n",
    "                )\n",
    "        WHERE rn = 1)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:12:40.244274Z",
     "start_time": "2023-03-23T16:12:38.415219Z"
    }
   },
   "outputs": [],
   "source": [
    "create_view(device_view_query, 'v_device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:00:36.112394Z",
     "start_time": "2023-03-23T16:00:36.068390Z"
    }
   },
   "outputs": [],
   "source": [
    "## QC: CHECK IF THERE ARE DUP ROWS\n",
    "df = pd.read_gbq('''\n",
    "        SELECT * EXCEPT (rn, upload_date)\n",
    "        FROM (\n",
    "          SELECT \n",
    "            m.participant_id AS person_id, \n",
    "            t.* EXCEPT (vibrent_id),\n",
    "            ROW_NUMBER() OVER(PARTITION BY vibrent_id, date ORDER BY upload_date DESC) AS rn\n",
    "          FROM `{dataset_name}.device` t\n",
    "          JOIN `{dataset_name}.id_mapping` m USING(vibrent_id)\n",
    "        )\n",
    "        WHERE rn = 1 \n",
    "        ''')\n",
    "\n",
    "n_rows = df.reset_index().groupby(df.columns.to_list()).nunique()\n",
    "n_rows.columns = ['n_rows']\n",
    "n_rows['n_rows'] = n_rows['n_rows'].astype('int64')\n",
    "n_rows[n_rows.n_rows >1]# = n_rows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
