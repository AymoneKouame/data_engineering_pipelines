{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook saves R functions (even though it is a py notebook) to the workspace bucket that can be called from any notebook in the workspace.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to save a file to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "def load_to_bucket(source_filename, destination_blob_name = 'notebooks/all_x_all'):\n",
    "    \n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "    args = [\"gsutil\", \"cp\", \"-R\", f\"./{source_filename}\", f\"{my_bucket}/{destination_blob_name}/{source_filename}\"]\n",
    "    output = subprocess.run(args, capture_output=True)\n",
    "    print(output.stderr)\n",
    "    print(f'\\n file in bucket at {destination_blob_name}/{source_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "dataset = os.getenv('WORKSPACE_CDR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# import glob\n",
    "\n",
    "# notebooks_directory = \"/path/to/your/notebooks/\"\n",
    "# output_directory = \"/path/to/your/output/\"\n",
    "\n",
    "# for notebook in glob.glob(f\"{notebooks_directory}/**/*.ipynb\", recursive=True):\n",
    "#     output_notebook = os.path.join(output_directory, os.path.basename(notebook))\n",
    "#     command = [\"jupyter\", \"nbconvert\", \"--to\", \"notebook\", \"--execute\", notebook, \"--output\", output_notebook]\n",
    "#     subprocess.run(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_from_bucket(name_of_file_in_bucket, directory = 'notebooks/all_x_all', remove_dot = 'no'):\n",
    "    my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "    if remove_dot == 'no':\n",
    "        df = pd.read_csv(f\"gsutil cp '{my_bucket}/{directory}/{name_of_file_in_bucket}' .\")\n",
    "    else:\n",
    "        df = pd.read_csv(f\"gsutil cp '{my_bucket}/{directory}/{name_of_file_in_bucket}'\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_csv_from_bucket('r_drug_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `allxall_pm_summary_functions.R`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%%writefile allxall_pm_summary_functions.R\n",
    "\n",
    "##################################################### SET UP ##############################################################\n",
    "package_list <- c('bigrquery','tidyverse','dplyr','janitor','data.table')\n",
    "for (pkg in package_list) if(!pkg %in% installed.packages()) {install.packages(pkg, quiet = T)}\n",
    "\n",
    "library(bigrquery, warn.conflicts = F, quietly = T)\n",
    "library(tidyverse, warn.conflicts = F, quietly = T)\n",
    "library(janitor, warn.conflicts = F, quietly = T)\n",
    "library(dplyr, warn.conflicts = F, quietly = T)\n",
    "library(data.table, warn.conflicts = F, quietly = T)\n",
    "options(dplyr.summarise.inform = FALSE)\n",
    "\n",
    "read_csv_cols_from_bucket <- function(directory = 'notebooks/all_x_all/', name_of_file_in_bucket, concept_id\n",
    "                                      , remove_dot = 'no'){\n",
    "    \n",
    "    #reads person_id and concept id columns in a df from the input csv file in teh bucket\n",
    "    #if input csv file is already read from the bucket to the env, it simply reads the person_id and concept id columns in a df\n",
    "    my_bucket = Sys.getenv('WORKSPACE_BUCKET')\n",
    "    \n",
    "    if (file.exists(name_of_file_in_bucket)){my_dataframe  <- fread(name_of_file_in_bucket\n",
    "                                                                  , select = c(\"person_id\", str_glue(\"{concept_id}\")))\n",
    "    } else {\n",
    "            if (remove_dot == 'no'){\n",
    "                    system(paste0(\"gsutil cp \", my_bucket, \"/\", directory, name_of_file_in_bucket, \" .\"), intern=T)              \n",
    "            } else {system(paste0(\"gsutil cp \", my_bucket, \"/\", directory, name_of_file_in_bucket), intern=T)}\n",
    "        \n",
    "            my_dataframe  <- fread(name_of_file_in_bucket, select = c(\"person_id\", str_glue(\"{concept_id}\")))\n",
    "            }\n",
    "\n",
    "    return(my_dataframe)\n",
    "    }\n",
    "\n",
    "read_csv_from_bucket <- function(directory = 'notebooks/all_x_all/', name_of_file_in_bucket, remove_dot = 'no'){\n",
    "    \n",
    "    #reads person_id and concept id columns in a df from the input csv file in teh bucket\n",
    "    #if input csv file is already read from the bucket to the env, it simply reads the person_id and concept id columns in a df\n",
    "    my_bucket = Sys.getenv('WORKSPACE_BUCKET')\n",
    "    \n",
    "    if (file.exists(name_of_file_in_bucket)){my_dataframe  <- fread(name_of_file_in_bucket)\n",
    "    } else {\n",
    "            if (remove_dot == 'no'){\n",
    "                    system(paste0(\"gsutil cp \", my_bucket, \"/\", directory, name_of_file_in_bucket, \" .\"), intern=T)              \n",
    "            } else {system(paste0(\"gsutil cp \", my_bucket, \"/\", directory, name_of_file_in_bucket), intern=T)}\n",
    "        \n",
    "            my_dataframe  <- fread(name_of_file_in_bucket)\n",
    "            }\n",
    "    my_dataframe  <- as.data.frame(my_dataframe)\n",
    "    return(my_dataframe)\n",
    "    }\n",
    "\n",
    "                      \n",
    "barplot <- function(df, plot_title, n_col, X, Y = `Percentage`, Fill = \"grey\", Facet = \"Cases\"\n",
    "                    , facet_nrow = NULL, facet_col = NULL, base_text_size =12, w = 14, h = 8){\n",
    "    \n",
    "    df[\"Percentage\"] <- round((df[n_col]/df['numerator'])*100,2)\n",
    "    df[\"Label\"] <- paste0(format(df[[n_col]], big.mark=\",\"), ' (', df$Percentage, '%)')    \n",
    "\n",
    "    if (!is.null(Facet)){df[\"Facet\"] <- paste0(df[[Facet]], 'N=', format(df$numerator, big.mark=\",\"), ' (100%)')}\n",
    "    \n",
    "    ###### Plot #####\n",
    "\n",
    "    options(repr.plot.width = w, repr.plot.height = h) #~1.5 per bar\n",
    "    p <- ggplot(data=df, aes(x={{X}}, y={{Y}}#, fill = {{Fill}}\n",
    "                            )) +\n",
    "            geom_bar(stat=\"identity\", position = 'dodge2', fill = Fill) +\n",
    "            geom_text(aes(label= Label), hjust=\"inward\", vjust = 0.5, size=(base_text_size/3)+1\n",
    "                      , position = position_dodge(width = 0.9)) +\n",
    "            labs(x = '', y = '', title = plot_title) +\n",
    "            theme_minimal()+ \n",
    "            theme(axis.text.x = element_blank(), axis.text.y = element_text(size = base_text_size+4)\n",
    "                  , legend.title = element_blank()\n",
    "                  , legend.text = element_text(size = base_text_size+5)\n",
    "                  , legend.position = \"top\", legend.box = \"horizontal\"\n",
    "                  , plot.title = element_text(hjust = 0.5)\n",
    "                  , title = element_text(size = base_text_size+4)) +\n",
    "            coord_flip()\n",
    "  \n",
    "    if (!is.null(Facet)){\n",
    "        p <- p+facet_wrap(~Facet, nrow = facet_nrow, ncol = facet_col)+\n",
    "                theme(strip.text.x = element_text(size = base_text_size+6))+theme(legend.position = \"none\")\n",
    "            }\n",
    "    return(p)\n",
    "    }\n",
    "\n",
    "##################################################DATA ############################################################\n",
    "system2('gsutil',args = c('cp','gs://fc-aou-preprod-datasets-controlled/v7/wgs/without_ext_aian_prod/vds/aux/ancestry/delta_v1_gt_no_ext_aian_gq0_prod.ancestry_preds.tsv','./ancestry.tsv'))\n",
    "ancestry_df = read_tsv('ancestry.tsv', col_types='ic-c-') %>% rename(person_id=research_id, ancestry = ancestry_pred)\n",
    "ancestry_df$ancestry = toupper(ancestry_df$ancestry)\n",
    "\n",
    "demographics_df <- read_csv_from_bucket(name_of_file_in_bucket = 'demographics_table.csv') %>% rename(age_group_at_cdr=age_group)\n",
    "\n",
    "################################# physical measurements specific functions #########################################\n",
    "#physical_measurement_table <- read_csv_from_bucket(name_of_file_in_bucket = 'physical_measurement_table.csv')\n",
    "continuous_data_summary <- function(df_measurement){\n",
    "    \n",
    "    value_column = colnames(select(df_measurement, -c('person_id')))\n",
    "    cat(paste0('Histogram and Descriptive Statistics of ', value_column))\n",
    "    \n",
    "    #df_measurement <- drop_na(df_measurement)\n",
    "    stats_df <- df_measurement %>%\n",
    "            dplyr::summarise('Mean' = mean(df_measurement[[value_column]], na.rm = TRUE)\n",
    "                             ,'Min' = min(df_measurement[[value_column]], na.rm = TRUE)\n",
    "                             ,'Max' = max(df_measurement[[value_column]], na.rm = TRUE)\n",
    "                             ,'1% Quantile' = quantile(df_measurement[[value_column]], 0.01, na.rm = TRUE)\n",
    "                             ,'2% Quantile' = quantile(df_measurement[[value_column]], 0.02, na.rm = TRUE)\n",
    "                             ,'25th Quantile' = quantile(df_measurement[[value_column]], 0.25, na.rm = TRUE)\n",
    "                             ,'50th Quantile (Median)' = quantile(df_measurement[[value_column]], 0.5, na.rm = TRUE)\n",
    "                             ,'75th Quantile' = quantile(df_measurement[[value_column]], 0.75, na.rm = TRUE)\n",
    "                             ,'98th Quantile' = quantile(df_measurement[[value_column]], 0.98, na.rm = TRUE)\n",
    "                             ,'99th Quantile' = quantile(df_measurement[[value_column]], 0.99, na.rm = TRUE)\n",
    "                             ,'Standard Deviation' = sd(df_measurement[[value_column]], na.rm = TRUE)\n",
    "                            )\n",
    "    options(repr.plot.width = 8, repr.plot.height = 6)\n",
    "    hist(x = df_measurement[[value_column]], main= str_glue('Histogram of {value_column}'), xlab= \"\") #paste0(value_column, \" values\")\n",
    "    \n",
    "    df_plot <- df_measurement\n",
    "    boxplot(df_plot[[value_column]], main = str_glue('Boxplot of {value_column}'), xlab=\"\")  \n",
    "    View(stats_df)\n",
    "    \n",
    "    return(stats_df)\n",
    "}\n",
    "\n",
    "wrangle_cont_data <- function(data, demog_vars, num_col = 'name'){\n",
    "    remove_cols <- append('person_id',demog_vars)\n",
    "    pivot_cols <- unlist(colnames(data))\n",
    "    pivot_cols <- pivot_cols[pivot_cols %in% remove_cols == FALSE]\n",
    "    \n",
    "    \n",
    "    data_long <- pivot_longer(data, cols = all_of(pivot_cols))\n",
    "    data_long <- subset(data_long[!is.na(data_long$value),], select = -c(value))\n",
    "    \n",
    "    groupby_cols = unlist(append(demog_vars, 'name'))\n",
    "    data_long_count <- data_long%>%group_by(across(all_of(groupby_cols)))%>%summarize(n_pids = n_distinct(person_id))\n",
    "    \n",
    "    if (num_col == 'overall'){ #For overall counts\n",
    "        num <- n_distinct(data$person_id)\n",
    "        data_long_count$numerator <- num}\n",
    "    else{\n",
    "        num_df <- data_long[c('person_id',num_col)]%>%group_by(across(all_of(num_col)))%>%\n",
    "                  summarize(numerator = n_distinct(person_id))\n",
    "        data_long_count <- left_join(data_long_count, num_df, by = num_col)\n",
    "        }\n",
    "    \n",
    "    return(data_long_count)\n",
    "    }\n",
    "\n",
    "demographic_data_summary <- function(df_measurement){\n",
    "#     df_measurement = read_csv_cols_from_bucket(name_of_file_in_bucket = 'physical_measurement_table.csv'\n",
    "#                                                 , concept_id = concept_id)\n",
    "#     title = str_to_upper(gsub('-',' ', concept_id))\n",
    "     \n",
    "#     cat(str_glue('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ {title} Summary ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'))\n",
    "#     cat(\"\\n\\n\")\n",
    "#     continuous_data_summary(df_measurement)\n",
    "    \n",
    "    # ~~~Demographics~~~\n",
    "    # Age\n",
    "    age_pm_long_count<- wrangle_cont_data(data = inner_join(df_measurement, demographics_df[c('person_id','age_group_at_cdr')]\n",
    "                                                            , by = 'person_id'), demog_var = 'age_group_at_cdr')\n",
    "    View(barplot(df = age_pm_long_count, X = `age_group_at_cdr`, n_col = 'n_pids'\n",
    "                 , plot_title = '\\n\\n~~~Demographics~~~\\n\\n\\n\\nAge at CDR')+ guides(fill=\"none\"))\n",
    "\n",
    "    # Sex at Birth\n",
    "    sex_pm_long_count<- wrangle_cont_data(data = inner_join(df_measurement, demographics_df[c('person_id','sex_at_birth')]\n",
    "                                                          , by = 'person_id'), demog_vars = 'sex_at_birth')\n",
    "    View(barplot(df = sex_pm_long_count, X = `sex_at_birth`, n_col = 'n_pids'\n",
    "                 , plot_title = 'Sex at Birth', h = 5)+ guides(fill=\"none\"))\n",
    "\n",
    "\n",
    "    # Ancestry\n",
    "    ancestry_pm_long_count<- wrangle_cont_data(data = inner_join(df_measurement, ancestry_df[c('person_id','ancestry')]\n",
    "                                                                 , by = 'person_id'), demog_vars = 'ancestry')\n",
    "    View(barplot(df = ancestry_pm_long_count, X = `ancestry`, n_col = 'n_pids'\n",
    "                 , plot_title = 'Ancestry\\n', h = 5)+ guides(fill=\"none\"))\n",
    "\n",
    "\n",
    "    # Sex at birth and Ancestry\n",
    "    ancestry_sex_m <- inner_join(df_measurement, demographics_df[c('person_id','sex_at_birth')], by = 'person_id')\n",
    "    ancestry_sex_m <- inner_join(ancestry_sex_m, ancestry_df[c('person_id','ancestry')], by = 'person_id')\n",
    "    ancestry_sex_m_long_count<- wrangle_cont_data(data = ancestry_sex_m, demog_vars = list('sex_at_birth','ancestry')\n",
    "                                                   , num_col = 'ancestry')\n",
    "\n",
    "    View(barplot(df = ancestry_sex_m_long_count, X = `sex_at_birth`, n_col = 'n_pids', Facet = \"ancestry\"\n",
    "                , plot_title = 'Sex at Birth & Ancestry\\n\\n'\n",
    "                , facet_nrow = 4, facet_col = 2, h = 10))\n",
    "\n",
    "    # Age at CDR and Ancestry\n",
    "    ancestry_age_m <- inner_join(df_measurement, demographics_df[c('person_id','age_group_at_cdr')], by = 'person_id')\n",
    "    ancestry_age_m <- inner_join(ancestry_age_m, ancestry_df[c('person_id','ancestry')], by = 'person_id')\n",
    "    ancestry_age_m_long_count<- wrangle_cont_data(data = ancestry_age_m, demog_vars = list('age_group_at_cdr','ancestry')\n",
    "                                                   , num_col = 'ancestry')\n",
    "    View(barplot(df = ancestry_age_m_long_count, X = `age_group_at_cdr`, n_col = 'n_pids', Facet = \"ancestry\"\n",
    "                , plot_title = 'Age at CDR & Ancestry\\n\\n'\n",
    "                , facet_nrow = 4, facet_col = 2, h = 15))\n",
    "     \n",
    " }\n",
    "\n",
    "pm_data_summary <- function(concept_id){\n",
    "    df_measurement = read_csv_cols_from_bucket(name_of_file_in_bucket = 'physical_measurement_table.csv'\n",
    "                                                , concept_id = concept_id)\n",
    "    title = str_to_upper(gsub('-',' ', concept_id))\n",
    "     \n",
    "    cat(str_glue('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ {title} Summary ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'))\n",
    "    cat(\"\\n\\n\")\n",
    "    continuous_data_summary(df_measurement)\n",
    "    \n",
    "    # ~~~Demographics~~~\n",
    "    demographic_data_summary(df_measurement)\n",
    "     \n",
    " }\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_file = 'allxall_pm_summary_functions.R'\n",
    "load_to_bucket(source_filename = r_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `allxall_lab_summary_functions.R`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile allxall_lab_summary_functions.R\n",
    "\n",
    "package_list <- c('bigrquery','tidyverse','dplyr','janitor','data.table')\n",
    "for (pkg in package_list) if(!pkg %in% installed.packages()) {install.packages(pkg, quiet = T)}\n",
    "\n",
    "library(bigrquery, warn.conflicts = F, quietly = T)\n",
    "library(tidyverse, warn.conflicts = F, quietly = T)\n",
    "library(janitor, warn.conflicts = F, quietly = T)\n",
    "library(dplyr, warn.conflicts = F, quietly = T)\n",
    "library(data.table, warn.conflicts = F, quietly = T)\n",
    "options(dplyr.summarise.inform = FALSE)\n",
    "\n",
    "bq <- function(query) {\n",
    "    bq_table_download(bq_project_query(Sys.getenv('GOOGLE_PROJECT'), page_size = 25000,\n",
    "                                       query=query, default_dataset = Sys.getenv('WORKSPACE_CDR')))\n",
    "    }\n",
    "\n",
    "\n",
    "lab_data <- function(ancestor_cid){\n",
    "    \n",
    "    df <- bq(str_glue(\"\n",
    "                SELECT DISTINCT person_id\n",
    "                , measurement_concept_id\n",
    "                , LOWER(cm.concept_name) as measurement_concept_name, LOWER(cam.concept_name) as ancestor_concept_name\n",
    "                , value_as_number\n",
    "                , range_low, range_high\n",
    "                , unit_concept_id, LOWER(u.concept_name) as unit_concept_name\n",
    "                , value_as_concept_id, LOWER(cv.concept_name) as value_as_concept_name\n",
    "                , operator_concept_id, LOWER(co.concept_name) as operator_concept_name \n",
    "                , measurement_datetime --, measurement_date\n",
    "                , src_id ###, MAX(measurement_date) AS most_recent_measurement_date\n",
    "                \n",
    "                #see note 2 - only use those when the standard concepts are not available/do not make sense\n",
    "                , measurement_source_concept_id, LOWER(measurement_source_value) AS measurement_source_value\n",
    "                , LOWER(unit_source_value) AS unit_source_value\n",
    "                #, CASE WHEN value_as_number IS NOT NULL THEN 'numeric measurement' \n",
    "                       #WHEN value_as_number IS NULL and (value_as_concept_id IS NOT NULL OR value_as_concept_id !=0)\n",
    "                            #THEN 'categorical measurement' \n",
    "                      # ELSE 'other' END as measurement_category\n",
    "\n",
    "            FROM `measurement` m \n",
    "            JOIN `measurement_ext` m_ext ON m.measurement_id = m_ext.measurement_id\n",
    "            JOIN `concept_ancestor` ON descendant_concept_id = measurement_concept_id\n",
    "            JOIN `concept` as cm on cm.concept_id = measurement_concept_id\n",
    "            JOIN `concept` as cam on cam.concept_id = ancestor_concept_id\n",
    "            LEFT JOIN (SELECT c2.concept_name, c1.concept_id \n",
    "                    FROM `concept` c1 JOIN `concept` c2 on c1.concept_name = c2.concept_code\n",
    "                    WHERE c1.domain_id = 'Meas Value') as cv on cv.concept_id = value_as_concept_id            \n",
    "            LEFT JOIN `concept` as co on co.concept_id = operator_concept_id\n",
    "            LEFT JOIN `concept` as u on u.concept_id = unit_concept_id\n",
    "                      \n",
    "            WHERE LOWER(m_ext.src_id) LIKE '%ehr%' AND ancestor_concept_id IN ({ancestor_cid})\n",
    "                      AND value_as_number IS NOT NULL\n",
    "            \n",
    "            \"))\n",
    "    filename = str_glue('measurement_{ancestor_cid}.csv')\n",
    "    print(str_glue(\"\\nFinal output will be saved to bucket as {filename}\\n\"))\n",
    "    \n",
    "    \n",
    "    return(df)\n",
    "    }\n",
    "                      \n",
    "simple_boxplot<- function(measurement_df, title = '', ancestor_concept_name, w = 20, h = 8){   \n",
    "    \n",
    "     #units = paste0(unique(measurement_df$unit_concept_name),collapse = ', ')\n",
    "    #title = paste0(str_to_upper(title), \"\\n\", str_glue(\"Units: {units}\"))\n",
    "    \n",
    "    options(repr.plot.width = w, repr.plot.height = h)\n",
    "    \n",
    "    measurement_df$scr_id <- gsub('EHR site ', '#', measurement_df$src_id) \n",
    "    boxplot(value_as_number~src_id, data=measurement_df,main=title, xlab=\"EHR Site\"\n",
    "            , ylab= str_glue(\"{ancestor_concept_name} Values\"))\n",
    "    \n",
    "    }\n",
    "                      \n",
    "\n",
    "simple_histogram<- function(measurement_df, value_col = 'value_as_number'\n",
    "                            ,title = '', ancestor_concept_name, w = 15, h = 8){   \n",
    "    \n",
    "    #units = paste0(unique(measurement_df$unit_concept_name),collapse = ', ')\n",
    "    #title = paste0(str_to_upper(title), \"\\n\", str_glue(\"Units: {units}\"))\n",
    "    \n",
    "    options(repr.plot.width = w, repr.plot.height = h)\n",
    "    hist(measurement_df[[value_col]], main = title, xlab= str_glue(\"{ancestor_concept_name} Values\"))\n",
    "    \n",
    "    }\n",
    "\n",
    "stats_table <- function(df_measurement){\n",
    "    \n",
    "    #value_column = 'value_as_number'\n",
    "    \n",
    "    #df_measurement <- drop_na(df_measurement)\n",
    "    stats_df <- df_measurement %>%\n",
    "            dplyr::group_by(src_id) %>%\n",
    "            dplyr::summarize('Mean' = mean(value_as_number, na.rm = TRUE)\n",
    "                             ,'Min' = min(value_as_number, na.rm = TRUE)\n",
    "                             ,'Max' = max(value_as_number, na.rm = TRUE)\n",
    "                             ,'1% Quantile' = quantile(value_as_number, 0.01, na.rm = TRUE)\n",
    "                             ,'2% Quantile' = quantile(value_as_number, 0.02, na.rm = TRUE)\n",
    "                             ,'25th Quantile' = quantile(value_as_number, 0.25, na.rm = TRUE)\n",
    "                             ,'50th Quantile (Median)' = quantile(value_as_number, 0.5, na.rm = TRUE)\n",
    "                             ,'75th Quantile' = quantile(value_as_number, 0.75, na.rm = TRUE)\n",
    "                             ,'98th Quantile' = quantile(value_as_number, 0.98, na.rm = TRUE)\n",
    "                             ,'99th Quantile' = quantile(value_as_number, 0.99, na.rm = TRUE)\n",
    "                             ,'Standard Deviation' = sd(value_as_number, na.rm = TRUE)\n",
    "                            ) %>%\n",
    "            rename('EHR Site' = src_id)\n",
    "    \n",
    "    return(stats_df)\n",
    "}\n",
    "                      \n",
    "final_dataframe <- function(measurement_df, concept_id){\n",
    "    latest_df <- measurement_df[c('person_id', 'measurement_datetime', 'value_as_number', 'unit_concept_name')] %>%\n",
    "                        dplyr::group_by(person_id) %>%\n",
    "                        filter(measurement_datetime == max(measurement_datetime)) %>%\n",
    "                        summarize(value_as_number = paste0(value_as_number,  collapse = ', ')\n",
    "                                 , unit_concept_name = paste0(unit_concept_name,  collapse = ', ')) %>%\n",
    "                        rename(latest_value = value_as_number, latest_value_unit = unit_concept_name)\n",
    "\n",
    "    count_df <- drop_na(measurement_df[c('person_id','value_as_number')]) %>%\n",
    "                dplyr::group_by(person_id) %>%\n",
    "                dplyr::summarize('values_count' = n_distinct(value_as_number, na.rm = TRUE))\n",
    "\n",
    "    output_df <- measurement_df[c('person_id','value_as_number')] %>%\n",
    "                dplyr::group_by(person_id) %>%\n",
    "                dplyr::summarize('Mean' = mean(value_as_number, na.rm = TRUE)\n",
    "                                 ,'Min' = min(value_as_number, na.rm = TRUE)\n",
    "                                 ,'Max' = max(value_as_number, na.rm = TRUE)\n",
    "                                 ,'1% Quantile' = quantile(value_as_number, 0.01, na.rm = TRUE)\n",
    "                                 ,'2% Quantile' = quantile(value_as_number, 0.02, na.rm = TRUE)\n",
    "                                 ,'25th Quantile' = quantile(value_as_number, 0.25, na.rm = TRUE)\n",
    "                                 ,'50th Quantile (Median)' = quantile(value_as_number, 0.5, na.rm = TRUE)\n",
    "                                 ,'75th Quantile' = quantile(value_as_number, 0.75, na.rm = TRUE)\n",
    "                                 ,'98th Quantile' = quantile(value_as_number, 0.98, na.rm = TRUE)\n",
    "                                 ,'99th Quantile' = quantile(value_as_number, 0.99, na.rm = TRUE)\n",
    "                                 ,'Standard Deviation' = sd(value_as_number, na.rm = TRUE)\n",
    "                                ) %>%\n",
    "                merge(latest_df) %>%\n",
    "                merge(count_df)\n",
    "    \n",
    "    #### final dataframe output\n",
    "    filename = str_glue('measurement_{concept_id}.csv')\n",
    "    write_csv(output_df, filename)\n",
    "    \n",
    "    my_bucket = Sys.getenv('WORKSPACE_BUCKET')\n",
    "    directory = 'notebooks/all_x_all/'\n",
    "    \n",
    "    system(paste0(\"gsutil cp \", filename, \" \", my_bucket, \"/\", directory, filename), intern=T)\n",
    "    system(paste0(\"gsutil ls \", my_bucket, \"/\", directory, filename), intern=T)\n",
    "    \n",
    "\n",
    "    return(output_df) \n",
    "    }\n",
    "                      \n",
    "# lab_summary <- function(measurement_df, ancestor_concept_name){\n",
    "    \n",
    "#     as.data.frame(stats_table(measurement_df))\n",
    "    \n",
    "#     simple_histogram(measurement_df, ancestor_concept_name = ancestor_concept_name)\n",
    "    \n",
    "#     simple_boxplot(measurement_df, ancestor_concept_name = ancestor_concept_name)\n",
    "    \n",
    "#     return(df)\n",
    "#     }\n",
    "                      \n",
    "lab_data_summary <- function(concept_id){\n",
    "    df_measurement <- lab_data(ancestor_cid = concept_id)\n",
    "    \n",
    "    ancestor_concept_name = unique(df_measurement$ancestor_concept_name)\n",
    "    title = str_to_upper(str_glue('\\n\\n\\n~~~~~~~~~EHR {ancestor_concept_name} Summary & Distributions\\n\\n~~~~~~~~~~'))\n",
    "    \n",
    "    cat(title)\n",
    "    n_pids = n_distinct(df_measurement$person_id)\n",
    "    print(str_glue('N Pids :{n_pids}'))                     \n",
    "\n",
    "    #cat(\"\\n\\n\")\n",
    "    df_measurement$src_id<- gsub('EHR site ', '#', df_measurement$src_id)\n",
    "    \n",
    "    simple_histogram(df_measurement, ancestor_concept_name = ancestor_concept_name)\n",
    "    \n",
    "    simple_boxplot(df_measurement, ancestor_concept_name = ancestor_concept_name)\n",
    "    \n",
    "    #lab_summary(df_measurement, ancestor_concept_name)   \n",
    "    stats_df <- stats_table(df_measurement)\n",
    "    final_df <- final_dataframe(measurement_df = df_measurement, concept_id = concept_id)\n",
    "    stats_df\n",
    " }\n",
    "                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_file = 'allxall_lab_summary_functions.R'\n",
    "load_to_bucket(source_filename = r_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `allxall_cat_data_summary_functions.R`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile allxall_cat_data_summary_functions.R\n",
    "\n",
    "# print(\"All packages and custom functions loaded.\")\n",
    "# cat(\"                                                \")\n",
    "##################################################### SET UP ##############################################################\n",
    "package_list <- c('bigrquery','tidyverse','dplyr','janitor', 'data.table')\n",
    "for (pkg in package_list) if(!pkg %in% installed.packages()) {install.packages(pkg, quiet = T)}\n",
    "\n",
    "library(bigrquery, warn.conflicts = F, quietly = T)\n",
    "library(tidyverse, warn.conflicts = F, quietly = T)\n",
    "library(janitor, warn.conflicts = F, quietly = T)\n",
    "library(dplyr, warn.conflicts = F, quietly = T)\n",
    "library(data.table)\n",
    "options(dplyr.summarise.inform = FALSE)\n",
    "\n",
    "# the current workspace's dataset variable (to use in the query)\n",
    "dataset <- Sys.getenv('WORKSPACE_CDR')\n",
    "billing_project <- Sys.getenv('GOOGLE_PROJECT')\n",
    "my_bucket <- Sys.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "# helper function to make the download easier\n",
    "download_data <- function(query) {\n",
    "    tb <- bq_project_query(Sys.getenv('GOOGLE_PROJECT'), page_size = 25000,\n",
    "                           query = query, default_dataset = Sys.getenv('WORKSPACE_CDR'))\n",
    "    bq_table_download(tb)\n",
    "}\n",
    "\n",
    "# read_csv_from_bucket <- function(directory = 'notebooks/all_x_all/', name_of_file_in_bucket, remove_dot = 'no'){\n",
    "                \n",
    "#                 if (remove_dot == 'no'){\n",
    "#                     system(paste0(\"gsutil cp \", my_bucket, \"/\", directory, name_of_file_in_bucket, \" .\"), intern=T)\n",
    "#                 } else {\n",
    "#                     system(paste0(\"gsutil cp \", my_bucket, \"/\", directory, name_of_file_in_bucket), intern=T)\n",
    "#                     }\n",
    "                    \n",
    "#                 # Load the file into a dataframe\n",
    "#                 my_dataframe  <- read_csv(gsub('notebooks/all_x_all/','',name_of_file_in_bucket), show_col_types = FALSE)\n",
    "#                 return(my_dataframe)\n",
    "#             }\n",
    "\n",
    "read_csv_cols_from_bucket <- function(directory = 'notebooks/all_x_all/', name_of_file_in_bucket, concept_id, remove_dot = 'no'){\n",
    "    \n",
    "    #reads person_id and concept id columns in a df from the input csv file in teh bucket\n",
    "    #if input csv file is already read from the bucket to the env, it simply reads the person_id and concept id columns in a df\n",
    "    my_bucket = Sys.getenv('WORKSPACE_BUCKET')\n",
    "    \n",
    "    if (file.exists(name_of_file_in_bucket)){my_dataframe  <- fread(name_of_file_in_bucket\n",
    "                                                                  , select = c(\"person_id\", str_glue(\"{concept_id}\")))\n",
    "    } else {\n",
    "            if (remove_dot == 'no'){\n",
    "                    system(paste0(\"gsutil cp \", my_bucket, \"/\", directory, name_of_file_in_bucket, \" .\"), intern=T)              \n",
    "            } else {system(paste0(\"gsutil cp \", my_bucket, \"/\", directory, name_of_file_in_bucket), intern=T)}\n",
    "        \n",
    "            my_dataframe  <- fread(name_of_file_in_bucket, select = c(\"person_id\", str_glue(\"{concept_id}\")))\n",
    "            }   \n",
    "    my_dataframe  <- as.data.frame(my_dataframe)\n",
    "    return(my_dataframe)\n",
    "    }\n",
    "\n",
    "read_csv_from_bucket <- function(directory = 'notebooks/all_x_all/', name_of_file_in_bucket, remove_dot = 'no'){\n",
    "    \n",
    "    #reads person_id and concept id columns in a df from the input csv file in teh bucket\n",
    "    #if input csv file is already read from the bucket to the env, it simply reads the person_id and concept id columns in a df\n",
    "    my_bucket = Sys.getenv('WORKSPACE_BUCKET')\n",
    "    \n",
    "    if (file.exists(name_of_file_in_bucket)){my_dataframe  <- fread(name_of_file_in_bucket)\n",
    "    } else {\n",
    "            if (remove_dot == 'no'){\n",
    "                    system(paste0(\"gsutil cp \", my_bucket, \"/\", directory, name_of_file_in_bucket, \" .\"), intern=T)              \n",
    "            } else {system(paste0(\"gsutil cp \", my_bucket, \"/\", directory, name_of_file_in_bucket), intern=T)}\n",
    "        \n",
    "            my_dataframe  <- fread(name_of_file_in_bucket)\n",
    "            }\n",
    "    my_dataframe  <- as.data.frame(my_dataframe)\n",
    "    return(my_dataframe)\n",
    "    }\n",
    "\n",
    "\n",
    "barplot <- function(df, plot_title, n_col, X, Y = `Percentage`\n",
    "                             , Fill = \"Cases\", Facet = \"Cases\"\n",
    "                             , fill_palette = \"Pastel1\"\n",
    "                             , facet_nrow = 1, facet_col = 2, base_text_size =9, w = 14, h = 5){\n",
    "    \n",
    "    df[\"Percentage\"] <- round((df[n_col]/df['numerator'])*100,2)\n",
    "    df[\"Label\"] <- paste0(format(df[[n_col]], big.mark=\",\"), ' (', df$Percentage, '%)')    \n",
    "    #df$Legend <- factor(paste0('\\n',df[[Fill]], ': N=', format(df$numerator, big.mark=\",\"), ' (100%)'))\n",
    "    if (!is.null(Facet)){df[\"Facet\"] <- paste0('\\n\\n',df[[Facet]], '\\nN=', format(df$numerator, big.mark=\",\"), ' (100%)')}\n",
    "    ###### Plot #####\n",
    "    #X_string = deparse(substitute(X)); h*(n_distinct(df[X_string])\n",
    "    options(repr.plot.width = w, repr.plot.height = h) #~1.5 per bar\n",
    "    p <- ggplot(data=df, aes(x={{X}}, y={{Y}}, fill = {{Fill}}\n",
    "                            )) +\n",
    "            geom_bar(stat=\"identity\", position = 'dodge2') +\n",
    "            geom_text(aes(label= Label), hjust=\"inward\", vjust = 0.5, size=(base_text_size/3)+1\n",
    "                      , position = position_dodge(width = 0.9)) +\n",
    "            labs(x = '', y = '', title = plot_title) +\n",
    "            theme_minimal()+ \n",
    "            #scale_fill_grey(start = 0.7, end = 0.9) +\n",
    "            scale_fill_brewer(palette = fill_palette) +\n",
    "            #scale_fill_brewer()+\n",
    "            theme(axis.text.x = element_blank(), axis.text.y = element_text(size = base_text_size+4)\n",
    "                  , legend.title = element_blank()\n",
    "                  , legend.text = element_text(size = base_text_size+5)\n",
    "                  , legend.position = \"top\", legend.box = \"horizontal\"\n",
    "                  , plot.title = element_text(hjust = 0.5)\n",
    "                  , title = element_text(size = base_text_size+4)) +\n",
    "            #scale_fill_mn(labels = c(new_legend)) +\n",
    "            coord_flip()\n",
    "\n",
    "    \n",
    "    if (!is.null(Facet)){\n",
    "        p <- p+facet_wrap(~Facet, nrow = facet_nrow, ncol = facet_col)+\n",
    "                theme(strip.text.x = element_text(size = base_text_size+6))+theme(legend.position = \"none\")\n",
    "            }\n",
    "    return(p)\n",
    "    }\n",
    "\n",
    "############################ Phecode AND Survey data specific (cat dat with TRUE/FALSE (CASE/CONTROLS))\n",
    "\n",
    "wrangle_cat_data <- function(data_df){\n",
    "    colnames(data_df) = c('person_id', 'Cases')\n",
    "    data_df$Cases[data_df$Cases ==TRUE]<-'Cases'\n",
    "    data_df$Cases[data_df$Cases ==FALSE]<-'Controls'\n",
    "    #phecode_df$Cases[is.na(phecode_df$Cases)]<- 'NA'    \n",
    "    data_df <- data_df[!is.na(data_df$Cases),] # NEW removing NAs\n",
    "    return(data_df)\n",
    "    }\n",
    "\n",
    "count_by <- function(data_df, var, var_df){    \n",
    "\n",
    "    var_df = unique(var_df[c('person_id',var)])    \n",
    "    merged_df = inner_join(data_df, var_df, by = 'person_id') \n",
    "    counts_df <- merged_df %>% dplyr::group_by(merged_df[var], Cases) %>% \n",
    "                    dplyr::summarise('n_pids' = n_distinct(person_id))\n",
    "    \n",
    "    counts_df <- cbind(counts_df, numerator=NA)\n",
    "    counts_df$numerator[counts_df$Cases =='Cases'] <- n_distinct(merged_df$person_id[merged_df$Cases =='Cases'])\n",
    "    counts_df$numerator[counts_df$Cases =='Controls'] <- n_distinct(merged_df$person_id[merged_df$Cases =='Controls'])\n",
    "    counts_df$numerator[counts_df$Cases =='NA'] <- n_distinct(merged_df$person_id[merged_df$Cases =='NA'])\n",
    "    \n",
    "    return(counts_df)\n",
    "    }\n",
    "\n",
    "categorical_data_summary <- function(concept_id, name_of_file_in_bucket = 'pfhh_survey.csv'\n",
    "                                     , datatype = 'Survey', map_concept_name = TRUE){\n",
    "    # Function is ffor phecode, drug and survey data only (not measurements or physical measurements)\n",
    "    \n",
    "    system2('gsutil',args = c('cp','gs://fc-aou-preprod-datasets-controlled/v7/wgs/without_ext_aian_prod/vds/aux/ancestry/delta_v1_gt_no_ext_aian_gq0_prod.ancestry_preds.tsv','./ancestry.tsv'))        \n",
    "    # LOAD Data\n",
    "    \n",
    "    ## Demographics Data\n",
    "    demographics_df <- read_csv_from_bucket(name_of_file_in_bucket = 'demographics_table.csv')\n",
    "    ## Ancestry Data\n",
    "    ancestry_df = read_tsv('ancestry.tsv', col_types='ic-c-') %>% rename(person_id=research_id)\n",
    "    ancestry_df$ancestry_pred = toupper(ancestry_df$ancestry_pred)\n",
    "    ## Survey, drug or phecode Data\n",
    "    #concept_filename = str_glue('{base_filename}_{concept_id}.csv')\n",
    "    data_df = read_csv_cols_from_bucket(name_of_file_in_bucket = name_of_file_in_bucket, concept_id = concept_id)\n",
    "\n",
    "    # TRANSFORM DATA\n",
    "    Data_df <- wrangle_cat_data(data_df)\n",
    "    age_count_df <- count_by(Data_df, var= 'age_group', var_df = demographics_df)\n",
    "    \n",
    "    sex_count_df <- count_by(Data_df, var= 'sex_at_birth', var_df = demographics_df)\n",
    "    ancestry_count_df <- count_by(Data_df, var= 'ancestry_pred', var_df = ancestry_df)\n",
    "    sex_and_ancestry_counts_df <- count_by(Data_df, var= c('ancestry_pred','sex_at_birth')\n",
    "                                           , var_df = inner_join(ancestry_df[c('person_id','ancestry_pred')]\n",
    "                                                                 , demographics_df[c('person_id','sex_at_birth')]\n",
    "                                                                 , by = 'person_id'))\n",
    "\n",
    "    age_and_ancestry_counts_df <- count_by(Data_df, var= c('ancestry_pred','age_group')\n",
    "                                           , var_df = inner_join(ancestry_df[c('person_id','ancestry_pred')]\n",
    "                                                                 , demographics_df[c('person_id','age_group')]\n",
    "                                                                 , by = 'person_id'))\n",
    "\n",
    "    \n",
    "    ############################################################################################\n",
    "    if (map_concept_name == TRUE){\n",
    "        Map <- download_data(str_glue(\"SELECT concept_name FROM `{dataset}.concept` \\\n",
    "                                        WHERE concept_id = {concept_id} or concept_code = '{concept_id}'\"))\n",
    "        if (nrow(Map) == 0){concept_name = ''} \n",
    "        else {concept_name <- Map$concept_name; concept_name <- str_glue(' - {concept_name}')}\n",
    "      } else {concept_name = ''}   \n",
    "    concept_id = str_glue(\"{concept_id}{concept_name}\")\n",
    "    datatype = toupper(datatype)\n",
    "    \n",
    "    if (tolower(datatype) == 'pfhh'){\n",
    "        concept_id = str_glue('{concept_id}\\n(NB: Cases = Participants Who Reported Personally Having This Condition)')}\n",
    "    else {concept_id = concept_id}\n",
    "    \n",
    "    ##################################################PLOT SUMMARIES#######################################################\n",
    "    #'OVERALL SUMMARIES BY AGE, SEX AT BIRTH AND ANCESTRY\n",
    "    n = 1\n",
    "    View(barplot(age_count_df, n_col = \"n_pids\", X = `age_group`, h = 6\n",
    "                , plot_title= str_glue('{datatype} {concept_id}:\\n\\n\\n\\nOVERALL SUMMARIES BY AGE, SEX AT BIRTH AND ANCESTRY\\n\\n\\nFigure {n}: Age at CDR')))\n",
    "    n = n+1\n",
    "    View(barplot(sex_count_df, n_col = \"n_pids\", X = `sex_at_birth`, plot_title= str_glue('\\nFigure {n}: Sex at Birth')\n",
    "                    , fill_palette = \"Blues\"))#+theme(legend.position = \"none\")\n",
    "    n = n+1\n",
    "    View(barplot(ancestry_count_df, n_col = \"n_pids\", X = `ancestry_pred`, plot_title= str_glue('\\nFigure {n}: Ancestry')\n",
    "                     , fill_palette = \"Greens\"))\n",
    "    \n",
    "    \n",
    "    # DETAILED SUMMARIES BY SEX AT BIRTH AND ANCESTRY\n",
    "    sex_at_births = unique(sex_and_ancestry_counts_df$sex_at_birth)\n",
    "    n = n+1\n",
    "    var_sex1 = sex_at_births[1]\n",
    "    df_sex1 = sex_and_ancestry_counts_df[sex_and_ancestry_counts_df$sex_at_birth == var_sex1,]\n",
    "    View(barplot(df_sex1, n_col = \"n_pids\", X = `ancestry_pred`, Fill = `sex_at_birth`, fill_palette = \"Purples\"\n",
    "                     , plot_title= str_glue('DETAILED SUMMARIES BY SEX AT BIRTH AND ANCESTRY\\n\\n\\nFigure {n}: {var_sex1} (Sex at Birth) by Ancestry')\n",
    "                    , facet_nrow = 1, facet_col = 2))   \n",
    "    for (var in sex_at_births[-1]){\n",
    "        n = n+1\n",
    "        df_var = sex_and_ancestry_counts_df[sex_and_ancestry_counts_df$sex_at_birth == var,]\n",
    "        View(barplot(df_var, n_col = \"n_pids\", X = `ancestry_pred`, Fill = `sex_at_birth`, fill_palette = \"Purples\"\n",
    "                     , plot_title= str_glue('\\nFigure {n}: {var} by Ancestry')\n",
    "                    , facet_nrow = 1, facet_col = 2))\n",
    "        }\n",
    "\n",
    "    \n",
    "    # DETAILED SUMMARIES BY AGE AND ANCESTRY\n",
    "    ages = unique(age_and_ancestry_counts_df$age_group)\n",
    "    n = n+1\n",
    "    var_age1 = ages[1]\n",
    "    df_age1 = age_and_ancestry_counts_df[age_and_ancestry_counts_df$age_group == var_age1,]\n",
    "    View(barplot(df_age1, n_col = \"n_pids\", X = `ancestry_pred`, Fill = `age_group`, fill_palette = \"Pastel2\"\n",
    "                    , plot_title= str_glue('DETAILED SUMMARIES BY AGE AT CDR AND ANCESTRY\\n\\n\\nFigure {n}: {var_age1} Years Old by Ancestry')\n",
    "                        , facet_nrow = 1, facet_col = 2)) \n",
    "    for (var in ages[-1]){\n",
    "        n = n+1\n",
    "        df_var = age_and_ancestry_counts_df[age_and_ancestry_counts_df$age_group == var,]\n",
    "        View(barplot(df_var, n_col = \"n_pids\", X = `ancestry_pred`, Fill = `age_group`, fill_palette = \"Pastel2\"\n",
    "                         , plot_title= str_glue('\\nFigure {n}: {var} Years Old by Ancestry')\n",
    "                              , facet_nrow = 1, facet_col = 2))\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_file = 'allxall_cat_data_summary_functions.R'\n",
    "load_to_bucket(source_filename = r_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `allxall_lab_summary_functions.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile allxall_lab_summary_functions.py\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from google.cloud import storage\n",
    "\n",
    "import subprocess\n",
    "\n",
    "from plotnine import *\n",
    "import os\n",
    "dataset = os.getenv('WORKSPACE_CDR')\n",
    "my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "################################## load data that all notebooks use ##########################################\n",
    "demographics_df = read_csv_from_bucket(name_of_file_in_bucket = 'demographics_table.csv')\n",
    "ancestry_df = read_tsv('ancestry.tsv', col_types='ic-c-') %>% rename(person_id=research_id)\n",
    "ancestry_df$ancestry_pred = toupper(ancestry_df$ancestry_pred)\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "def load_to_bucket(source_filename, destination_blob_name = 'notebooks/all_x_all'):\n",
    "    \n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "    args = [\"gsutil\", \"cp\", \"-R\", f\"./{source_filename}\", f\"{my_bucket}/{destination_blob_name}/{source_filename}\"]\n",
    "    output = subprocess.run(args, capture_output=True)\n",
    "    print(output.stderr)\n",
    "    print(f'\\n file in bucket at {destination_blob_name}/{source_filename}')\n",
    "    \n",
    "    \n",
    "def measurement_data(ancestor_cid):\n",
    "    \n",
    "    start = datetime.now() #started 6:16 pm ish\n",
    "    \n",
    "    #print(cid)\n",
    "    df = pd.read_gbq(f\"\"\"\n",
    "            SELECT DISTINCT person_id\n",
    "                        , measurement_concept_id\n",
    "                        , measurement_source_concept_id, LOWER(measurement_source_value) AS measurement_source_value\n",
    "                        , value_as_number, range_low, range_high\n",
    "                        , LOWER(unit_source_value) AS unit_source_value , operator_concept_id\n",
    "                        , src_id\n",
    "                        , measurement_datetime\n",
    "                        ---, MAX(measurement_date) AS most_recent_measurement_date\n",
    "\n",
    "            FROM `{dataset}.measurement` m \n",
    "            JOIN `{dataset}.measurement_ext` m_ext ON m.measurement_id = m_ext.measurement_id\n",
    "            JOIN `{dataset}.concept_ancestor` ON descendant_concept_id = measurement_concept_id\n",
    "            AND ancestor_concept_id IN ({ancestor_cid})\n",
    "            WHERE m_ext.src_id LIKE '%EHR%' --GROUP BY 1,2,3,4,5,6,7,8,9 \n",
    "            \n",
    "            \"\"\")\n",
    "    end = datetime.now()\n",
    "    print(end - start)\n",
    "    n_pids = df.person_id.nunique()\n",
    "    \n",
    "    print(f'N Pids :{n_pids}')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def map_concept_names(ancestor_cid, merge = 'yes'):\n",
    "\n",
    "    df = pd.read_gbq(f\"\"\"\n",
    "        SELECT DISTINCT measurement_concept_id, LOWER(STANDARD_CONCEPT_NAME) AS measurement_concept_name\n",
    "        , measurement_source_concept_id, LOWER(SOURCE_CONCEPT_NAME) AS measurement_source_concept_name\n",
    "        , unit_concept_id, LOWER(unit_concept_name) AS unit_concept_name\n",
    "        , operator_concept_id, LOWER(operator_concept_name) AS operator_concept_name\n",
    "\n",
    "        FROM `{dataset}.ds_measurement` me\n",
    "        JOIN `{dataset}.concept_ancestor` ON descendant_concept_id = measurement_concept_id\n",
    "        AND ancestor_concept_id IN ({ancestor_cid}) \n",
    "        \"\"\")\n",
    "    return df\n",
    "\n",
    "def percentile(n):\n",
    "    def percentile_(x):\n",
    "        return np.percentile(x[~np.isnan(x)], n)\n",
    "    percentile_.__name__ = 'percentile_%s' % n\n",
    "    return percentile_\n",
    "\n",
    "\n",
    "def boxplot(measurement_df, c_name, fill_col, n_row= None, n_col = 1, w = 10, h = 12, facet_col = None):    \n",
    "    #measurement_df['meas_and_unit'] = measurement_df['measurement_concept_name']+ ' ('+measurement_df['unit_concept_name']+')'\n",
    "\n",
    "    plot = (ggplot(measurement_df, aes(x='src_id', y='value_as_number'#, fill=f'factor({fill_col})'\n",
    "               , color =f'factor({fill_col})')) \n",
    "            + geom_boxplot()\n",
    "            + labs(title = f\"Comparing distributions of EHR {c_name} measures\")\n",
    "            + theme(axis_text_x = element_text(angle = 90, hjust = 1))\n",
    "            + theme(figure_size=(w, h))\n",
    "           )\n",
    "    if facet_col is not None:     \n",
    "        if n_row is None:\n",
    "            n_row = measurement_df[facet_col].nunique()\n",
    "        else:\n",
    "            n_row = n_row\n",
    "        plot = (plot\n",
    "                + facet_wrap(facet_col, ncol = n_col, nrow = n_row, scales = 'free_y')\n",
    "               )\n",
    "             \n",
    "    return plot\n",
    "\n",
    "\n",
    "def check_df(measurement_df):\n",
    "    cols = ['measurement_concept_name', 'measurement_source_concept_name', 'unit_concept_name'\n",
    "        , 'operator_concept_name', 'value_as_number']\n",
    "\n",
    "    qc_df1 = measurement_df[['person_id']+cols].drop_duplicates()\n",
    "    qc_df1.loc[qc_df1.value_as_number.notna(), 'value_as_number'] = 'has value'\n",
    "    qc_df1.loc[qc_df1.value_as_number.isna(), 'value_as_number'] = 'no value'\n",
    "    qc_df1 = qc_df1.groupby(cols).nunique().sort_values('person_id')\n",
    "    qc_df1.columns = ['n_pids']\n",
    "    display(qc_df1)\n",
    "    \n",
    "    qc_df2 = measurement_df[['measurement_concept_name', 'value_as_number', 'unit_concept_name']].drop_duplicates()\\\n",
    "                .groupby(['measurement_concept_name', 'unit_concept_name'])\\\n",
    "                .agg({'value_as_number':['mean','median', 'min','max','std',percentile(1), percentile(25) , percentile(75), percentile(99)]})\n",
    "    qc_df2.columns = [c[1] for c in qc_df2.columns]\n",
    "    qc_df2 = qc_df2.reset_index()\n",
    "    display(qc_df2)\n",
    "    \n",
    "    return [qc_df1, qc_df2]\n",
    "\n",
    "\n",
    "def standard_cleaning(measurement_df): \n",
    "    print('''Standard Cleaning (for all measurements):\\n \n",
    "- Drop rows with:\n",
    "        - no value_as_number (can be done in SQL)\n",
    "        - no units or 'no matching concept units' or units that do not make sense (e.g. 45666662)\n",
    " (This will most likely drop some EHR sites)\\n \n",
    "- Harmonize units:\n",
    "        - e.g. mg/dl = milligram per deciliter, Percent = percent''')\n",
    "    \n",
    "    harmonize_units_dd = {'mg/dl':'milligram per deciliter', 'Percent':'percent'\n",
    "                          , 'meq/l':'milliequivalent per liter'}\n",
    "    measurement_clean_df = measurement_df[(measurement_df.value_as_number.notna()) & (measurement_df.unit_concept_id.notna())]\n",
    "    measurement_clean_df = measurement_clean_df[~(measurement_clean_df.unit_concept_name.isin(['no matching concept', 'no value']))]\n",
    "    measurement_clean_df['unit_concept_name'] = measurement_clean_df['unit_concept_name'].replace(harmonize_units_dd)\n",
    "    return measurement_clean_df\n",
    "\n",
    "def drop_extreme_outliers(df, max_percentage_diff_threshold, max_percentile_threshold\n",
    "                          , min_percentage_diff_threshold , min_percentile_threshold, c, u):\n",
    "    \n",
    "#     print(f'''Definition: Drop extreme outliers, which is defined as rows where:\n",
    "#     - the max measurement value is {max_percentage_diff_threshold}% larger than the maximum of all values at or below the {max_percentile_threshold} percentile. \n",
    "#     This threshold can be adjusted by using max_percentage_diff_threshold = n. The percentile threshold can also be adjusted using max_percentile_threshold = n.\n",
    "    \n",
    "#     - and/or the min measurement value is {min_percentile_threshold}% smaller than the min of all values at or below the {min_percentile_threshold} percentile\n",
    "#     . This threshold can be adjusted by using min_percentage_diff_threshold = n. The percentile threshold can also be adjusted using min_percentile_threshold = n.\\n\\n\\n''')\n",
    "    \n",
    "    # Drop extremely high values\n",
    "    values = sorted(df.value_as_number.dropna())\n",
    "    max_meas_value = max(values)\n",
    "    #diff = max_meas_value - np.percentile(values, max_percentile_threshold)\n",
    "    values_minus_max = [i for i in values if i < max_meas_value]\n",
    "    max_values_2nd = max(values_minus_max)\n",
    "    diff_perc1 = round(max_meas_value*100/max_values_2nd)\n",
    "    \n",
    "    if diff_perc1 >= max_percentage_diff_threshold:\n",
    "        print(f'Dropping {max_meas_value} ({u})')\n",
    "        #print(f'For {c} in {u}, the max measurement value ({max_meas_value}) is {diff_perc1}% larger than the maximum of all values at or below the 99 percentile ({max_values_2nd}).\\n We will consider it an extreme outlier and drop it.')\n",
    "        df_clean = df[df.value_as_number < max_meas_value]\n",
    "    else:\n",
    "        df_clean = df.copy()\n",
    "    \n",
    "    \n",
    "    # Drop extremely low values\n",
    "    min_meas_value = min(values)\n",
    "    #diff = min_meas_value - np.percentile(values, min_percentile_threshold)\n",
    "    values_minus_min = [i for i in values if i > min_meas_value]\n",
    "    min_values_2nd = min(values_minus_min)\n",
    "    diff_perc2 = abs(round(min_meas_value*100/min_values_2nd))\n",
    "    \n",
    "    if diff_perc2 >= min_percentage_diff_threshold:\n",
    "        print(f'Dropping {min_meas_value} ({u})')\n",
    "        #print(f'For {c} in {u}, the min measurement value ({min_meas_value}) is {diff_perc2}% smaller than the min of all values at or below the 1 percentile ({min_values_2nd}).\\n We will consider it an extreme outlier and drop it.')\n",
    "        df_clean2 = df_clean[df_clean.value_as_number > min_meas_value]\n",
    "    else:\n",
    "        df_clean2 = df_clean.copy()\n",
    "        \n",
    "    return df_clean2\n",
    "\n",
    "def drop_extreme_outliers_in_df(measurement_df, max_percentage_diff_threshold = 400, max_percentile_threshold = 99\n",
    "                          , min_percentage_diff_threshold = 400, min_percentile_threshold = 1):\n",
    "    measurement_clean2 = pd.DataFrame()\n",
    "    \n",
    "    print(f'''Definition: Drop extreme outliers, which is defined as rows where:\n",
    "    - the max measurement value is {max_percentage_diff_threshold}% larger than the maximum of all values at or below the {max_percentile_threshold} percentile. \n",
    "    This threshold can be adjusted by using max_percentage_diff_threshold = n. The percentile threshold can also be adjusted using max_percentile_threshold = n.\n",
    "    \n",
    "    - and/or the min measurement value is {min_percentile_threshold}% smaller than the min of all values at or below the {min_percentile_threshold} percentile\n",
    "    . This threshold can be adjusted by using min_percentage_diff_threshold = n. The percentile threshold can also be adjusted using min_percentile_threshold = n.\\n\\n\\n''')\n",
    "\n",
    "\n",
    "    for c in measurement_df.measurement_concept_name.unique():\n",
    "        DF1 = measurement_df[(measurement_df.measurement_concept_name == c)]\n",
    "        for u in DF1.unit_concept_name.unique():\n",
    "            DF2 = DF1[DF1.unit_concept_name ==u]\n",
    "            clean_DF2 = drop_extreme_outliers(DF2, max_percentage_diff_threshold = max_percentage_diff_threshold\n",
    "                                              , max_percentile_threshold = max_percentile_threshold\n",
    "                                              , min_percentage_diff_threshold = min_percentage_diff_threshold\n",
    "                                              , min_percentile_threshold = min_percentile_threshold, c= c, u = u)\n",
    "            measurement_clean2 = pd.concat([measurement_clean2, clean_DF2]) \n",
    "\n",
    "    return measurement_clean2\n",
    "\n",
    "def final_data_output(clean_measurement_df):\n",
    "    \n",
    "    base_cols = ['person_id', 'value_as_number']\n",
    "    df = clean_measurement_df[base_cols+['measurement_datetime']].drop_duplicates()\n",
    "\n",
    "    df_latest= df.loc[df.groupby('person_id')['measurement_datetime'].idxmax()]\n",
    "    df_latest = df_latest[base_cols].drop_duplicates()\n",
    "    df_latest.columns = ['person_id','latest']\n",
    "    df_latest = df_latest.reset_index(drop = True)\n",
    "\n",
    "    df_units = clean_measurement_df[['person_id','unit_concept_name']].drop_duplicates()\n",
    "    df_units['unit(s)'] = df_units.groupby('person_id')['unit_concept_name'].transform(lambda x: ', '.join(x.unique()))\n",
    "    df_units = df_units.drop(['unit_concept_name'],1)\n",
    "    df_units = df_units.drop_duplicates()\n",
    "    \n",
    "    df_stats = clean_measurement_df[base_cols]; df_stats[df_stats.value_as_number.notna()]    \n",
    "    df_stats = df_stats.groupby([c for c in base_cols if c != 'value_as_number'])\\\n",
    "                                .agg({'value_as_number':['min','median','max','mean', 'count']})\n",
    "    df_stats.columns = [c[1] for c in df_stats.columns]\n",
    "    df_stats = df_stats.reset_index()\n",
    "\n",
    "    df_final = df_stats.merge(df_units).merge(df_latest).drop_duplicates()\n",
    "    return df_final,df_units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload file to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload py file from cloud to bucket\n",
    "r_file = 'allxall_summary_functions.R'\n",
    "load_to_bucket(source_filename = r_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload individual concept id csv files to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfhh_survey_table = read_csv_from_bucket(name_of_file_in_bucket = 'pfhh_survey_table.csv')\n",
    "\n",
    "for c in pfhh_survey_table.drop('person_id',1).columns:\n",
    "    df = pfhh_survey_table[['person_id',c]].drop_duplicates()\n",
    "    question_filename = f'pfhh_survey_{c}.csv'\n",
    "    df.to_csv(question_filename, index = False)\n",
    "    load_to_bucket(source_filename = question_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_question_filename = f'pfhh_survey_{c}.csv'\n",
    "survey_question_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basefilename = 'drug'\n",
    "df_table = read_csv_from_bucket(name_of_file_in_bucket = 'r_drug_table.csv')\n",
    "for c in df_table.drop('person_id',1).columns:\n",
    "    df = df_table[['person_id',c]].drop_duplicates()\n",
    "    question_filename = f'drug_{c}.csv'\n",
    "    df.to_csv(question_filename, index = False)\n",
    "    load_to_bucket(source_filename = question_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import dask.dataframe as dd\n",
    "\n",
    "# #df_table = pd.read_csv('r_drug_table.csv') \n",
    "# #df_table = dd.read_csv('mcc2_phecode_table.csv')\n",
    "# for c in df_table.drop('person_id',1).columns:\n",
    "#     #question_filename = f'mcc2_phecode_{c}.csv'\n",
    "#     nb_filename = f'phecode_{c}_summary.ipynb'\n",
    "#     #os.remove(f\"{question_filename}\")\n",
    "#     os.remove(f\"{nb_filename}\")\n",
    "#     print(f\"{question_filename} deleted.\")\n",
    "#     print(f\"{nb_filename} deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drug = dd.read_csv('r_drug_table.csv')\n",
    "df_drug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_notebook(base_filename, phecode):\n",
    "    \n",
    "    code = ''' \n",
    "        ################################################## CODE ##############################################\n",
    "\n",
    "        # Set Up\n",
    "\n",
    "        ## Loading packages and custom functions for AllxAll Phenotypes Summaries\n",
    "\n",
    "        source_code_filename <- 'allxall_summary_functions.R'\n",
    "        system(paste0('gsutil cp ', Sys.getenv('WORKSPACE_BUCKET'), '/notebooks/all_x_all/', source_code_filename,  ' ./'), intern=T)\n",
    "        system2('gsutil',args = c('cp','gs://fc-aou-preprod-datasets-controlled/v7/wgs/without_ext_aian_prod/vds/aux/ancestry/delta_v1_gt_no_ext_aian_gq0_prod.ancestry_preds.tsv','./ancestry.tsv'))\n",
    "        source(source_code_filename)\n",
    "\n",
    "        # ################################################## SUMMARY ##############################################\n",
    "        \n",
    "        phecode_summary(phecode = '{phecode}')\n",
    "\n",
    "        '''\n",
    "    \n",
    "    notebook_name = f\"{base_filename}_{phecode}_summary\"\n",
    "    with open(f\"{notebook_name}.ipynb\", mode='a') as file:\n",
    "        file.write(code)\n",
    "        \n",
    "    print(f'{notebook_name}.ipynb saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_notebook(base_filename = 'phecode', phecode = '1830')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "295.99px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
